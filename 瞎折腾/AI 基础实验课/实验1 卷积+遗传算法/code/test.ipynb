{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d10edeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from d2l import torch as d2l\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1185e827",
   "metadata": {},
   "source": [
    "# 一、卷积模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f36202c",
   "metadata": {},
   "source": [
    "## 1. CIFA10 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccb7f3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 50000\n",
      "Number of test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    transform=transforms.ToTensor(),\n",
    "    download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    transform=transforms.ToTensor(),\n",
    "    download=True)\n",
    "\n",
    "print(f'Number of training samples: {len(train_dataset)}')\n",
    "print(f'Number of test samples: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6bbc253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([3, 32, 32])\n",
      "Label: 6\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = train_dataset[0]\n",
    "image, label = data\n",
    "print('Image shape:', image.shape)\n",
    "print('Label:', label)\n",
    "plt.imshow(image.permute(1, 2, 0))\n",
    "plt.title(f'Label: {label}')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5d8fdc",
   "metadata": {},
   "source": [
    "## 2. 参数可调的简单 CNN 实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98439f53",
   "metadata": {},
   "source": [
    "可调参数：\n",
    "| 参数               |       类型 | 取值范围             | 备注                        |\n",
    "| ---------------- | -------: | ---------------- | ------------------------- |\n",
    "| num_conv_layers  |       整数 | 1–5              | 有效卷积层数                    |\n",
    "| channels_base    |       整数 | 8–64（步长8）        | 第1层通道数                    |\n",
    "| kernel_size      |    列表/类别 | {1,3,5}          |                           |\n",
    "| pooling_type     |       类别 | {max, avg}       | 全局或按层                     |\n",
    "| use_batch_norm   |       布尔 | {0,1}            | 是否在 Conv 后使用 BatchNorm    |\n",
    "| use_dropout_conv |       布尔 | {0,1}            | 是否在卷积模块使用 Dropout2d       |\n",
    "| dropout_rate     |       浮点 | 0.0–0.7          | 仅当 use_dropout_conv=1 时有效 |\n",
    "| num_fc_layers    |       整数 | 1–3              | 全连接层数量                    |\n",
    "| fc_units         |   列表（每层） | 每项 64–2048（步长可选） | 未使用项设0                    |\n",
    "| lr               | 浮点（对数尺度） | 1e-5 – 1e-1      | 学习率                       |\n",
    "| batch_size       |       类别 | {32,64,128,256}  | 训练批次大小                    |\n",
    "| augmentation     |       布尔 | {0,1}            | 是否使用数据增强                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1c22fe",
   "metadata": {},
   "source": [
    "### CNN块构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6af54b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_layer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, \n",
    "                 kernel_size=3, stride=1, padding=1,\n",
    "                 is_batch_norm=False, pooling_type='max',\n",
    "                 is_dropout=False, dropout_rate=0.5):\n",
    "        super(CNN_layer, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        if pooling_type == 'max':\n",
    "            self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        elif pooling_type == 'avg':\n",
    "            self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        elif pooling_type == 'none':\n",
    "            self.pool = nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported pooling type. Use 'max', 'avg', or 'none'.\")\n",
    "\n",
    "        self.is_batch_norm = is_batch_norm\n",
    "        self.is_dropout = is_dropout\n",
    "        if self.is_batch_norm:\n",
    "            self.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "        if self.is_dropout:\n",
    "            self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.is_batch_norm:\n",
    "            x = self.batch_norm(x)\n",
    "        x = self.relu(x)\n",
    "        if self.is_dropout:\n",
    "            x = self.dropout(x)\n",
    "        x = self.pool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db9636b",
   "metadata": {},
   "source": [
    "(预留每层参数都能调的空间，但是懒得写了)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecf67c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_block(nn.Module):\n",
    "    def __init__(self, in_channels, num_layers=3,\n",
    "                 num_channels=[64, 128, 256],\n",
    "                 kernel_size=3, stride=1, padding=1,\n",
    "                 is_batch_norm=False, pooling_type='max',\n",
    "                 is_dropout=False, dropout_rate=0.5):\n",
    "        super(CNN_block, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            out_channels = num_channels[i]\n",
    "            layer = CNN_layer(\n",
    "                in_channels, out_channels,\n",
    "                kernel_size, stride, padding,\n",
    "                is_batch_norm, pooling_type,\n",
    "                is_dropout, dropout_rate)\n",
    "            layers.append(layer)\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.cnn = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355f63de",
   "metadata": {},
   "source": [
    "### Linear 层构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aea59199",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_layer(nn.Module):\n",
    "    def __init__(self, in_features, out_features,\n",
    "                 num_layers=2, hidden_size=[128, 64],\n",
    "                 is_dropout=False, dropout_rate=0.5,\n",
    "                 is_batch_norm=False):\n",
    "        super(Linear_layer, self).__init__()\n",
    "\n",
    "        self.linear = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                self.linear.append(nn.Linear(in_features, hidden_size[i]))\n",
    "            else:\n",
    "                self.linear.append(nn.Linear(hidden_size[i-1], hidden_size[i]))\n",
    "            self.linear.append(nn.ReLU())\n",
    "            if is_batch_norm:\n",
    "                self.linear.append(nn.BatchNorm1d(hidden_size[i]))\n",
    "            if is_dropout:\n",
    "                self.linear.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        self.out_layer = nn.Linear(hidden_size[-1], out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.out_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0958e473",
   "metadata": {},
   "source": [
    "### 完整模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07547552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "class CIFAR10_CNN(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=10,\n",
    "                 cnn_num_layers=5, cnn_num_channels=[64, 128, 256, 512, 512],\n",
    "                 cnn_is_batch_norm=False, cnn_pooling_type='max',\n",
    "                 cnn_is_dropout=True, cnn_dropout_rate=0.3,\n",
    "                 linear_num_layers=3, linear_hidden_size=[512, 256, 32],\n",
    "                 linear_is_dropout=False, linear_dropout_rate=0.5,\n",
    "                 linear_is_batch_norm=False,\n",
    "                 input_size=32):  # 新增 input_size 参数用于推断\n",
    "        super(CIFAR10_CNN, self).__init__()\n",
    "\n",
    "        self.cnn_block = CNN_block(\n",
    "            in_channels,\n",
    "            num_layers=cnn_num_layers,\n",
    "            num_channels=cnn_num_channels,\n",
    "            is_batch_norm=cnn_is_batch_norm,\n",
    "            pooling_type=cnn_pooling_type,\n",
    "            is_dropout=cnn_is_dropout,\n",
    "            dropout_rate=cnn_dropout_rate\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # 用虚拟张量推断展平后特征数（在 CPU 上，不改变模型参数）\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, in_channels, input_size, input_size)\n",
    "            feat = self.cnn_block(dummy)\n",
    "            n_flat = int(feat.shape[1] * feat.shape[2] * feat.shape[3])\n",
    "\n",
    "        self.linear_layer = Linear_layer(\n",
    "            in_features=n_flat,\n",
    "            out_features=num_classes,\n",
    "            num_layers=linear_num_layers,\n",
    "            hidden_size=linear_hidden_size,\n",
    "            is_dropout=linear_is_dropout,\n",
    "            dropout_rate=linear_dropout_rate,\n",
    "            is_batch_norm=linear_is_batch_norm\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn_block(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead6bcf7",
   "metadata": {},
   "source": [
    "测试模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bede11b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10_CNN(\n",
      "  (cnn_block): CNN_block(\n",
      "    (cnn): Sequential(\n",
      "      (0): CNN_layer(\n",
      "        (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (relu): ReLU()\n",
      "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout(p=0.3, inplace=False)\n",
      "      )\n",
      "      (1): CNN_layer(\n",
      "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (relu): ReLU()\n",
      "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout(p=0.3, inplace=False)\n",
      "      )\n",
      "      (2): CNN_layer(\n",
      "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (relu): ReLU()\n",
      "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout(p=0.3, inplace=False)\n",
      "      )\n",
      "      (3): CNN_layer(\n",
      "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (relu): ReLU()\n",
      "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout(p=0.3, inplace=False)\n",
      "      )\n",
      "      (4): CNN_layer(\n",
      "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (relu): ReLU()\n",
      "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout(p=0.3, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_layer): Linear_layer(\n",
      "    (linear): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=256, out_features=32, bias=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (out_layer): Linear(in_features=32, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CIFAR10_CNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2755fbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_block output shape: \t torch.Size([1, 512, 1, 1])\n",
      "Flatten output shape: \t torch.Size([1, 512])\n",
      "Linear_layer output shape: \t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "data = train_dataset[1]\n",
    "image, label = data\n",
    "image = image.unsqueeze(0)  # 添加 batch 维度\n",
    "\n",
    "# 输出完整前向传播过程\n",
    "for layer in model.children():\n",
    "    image = layer(image)\n",
    "    print(layer.__class__.__name__, 'output shape: \\t', image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cde9fb",
   "metadata": {},
   "source": [
    "## 3. 考虑数据增强的数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5105bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10(\n",
    "        is_train, augs, batch_size, sample_count=5000, shuffle_subset=True, seed=None):\n",
    "    dataset = torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\", train=is_train,\n",
    "        transform=augs, download=True\n",
    "    )\n",
    "    # 打乱后取前 sample_count 个样本（保证子集被随机采样）\n",
    "    if sample_count and is_train:\n",
    "        n = len(dataset)\n",
    "        if sample_count >= n:\n",
    "            indices = list(range(n))\n",
    "        else:\n",
    "            if seed is not None:\n",
    "                g = torch.Generator()\n",
    "                g.manual_seed(seed)\n",
    "                indices = torch.randperm(n, generator=g)[:sample_count].tolist()\n",
    "            else:\n",
    "                indices = torch.randperm(n)[:sample_count].tolist()\n",
    "        dataset = torch.utils.data.Subset(dataset, indices)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=is_train, num_workers=4\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a0923f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = load_cifar10(\n",
    "    is_train=True,\n",
    "    augs=transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    batch_size=64)\n",
    "\n",
    "test_loader = load_cifar10(\n",
    "    is_train=False,\n",
    "    augs=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a004ce51",
   "metadata": {},
   "source": [
    "## 4. 模型训练/测试函数构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f8291d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(model, X, y, loss, trainer, devices):\n",
    "    if isinstance(X, list):\n",
    "        X = [x.to(devices[0]) for x in X]\n",
    "    else:\n",
    "        X = X.to(devices[0])\n",
    "    y = y.to(devices[0])\n",
    "    model.train()\n",
    "    trainer.zero_grad()\n",
    "    pred = model(X)\n",
    "    l = loss(pred, y)\n",
    "    l.sum().backward()\n",
    "    trainer.step()\n",
    "    train_loss_sum = l.sum()\n",
    "    train_acc_sum = d2l.accuracy(pred, y)\n",
    "    return train_loss_sum, train_acc_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ed140f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_iter, test_iter, \n",
    "          loss, trainer, num_epochs, devices):\n",
    "    \"\"\"用多GPU进行模型训练\"\"\"\n",
    "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],\n",
    "                            legend=['train loss', 'train acc', 'test acc'])\n",
    "    model = nn.DataParallel(model, device_ids=devices).to(devices[0])\n",
    "    for epoch in range(num_epochs):\n",
    "        # 4个维度：储存训练损失，训练准确度，实例数，特点数\n",
    "        metric = d2l.Accumulator(4)\n",
    "        for i, (features, labels) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            l, acc = train_batch(\n",
    "                model, features, labels, loss, trainer, devices)\n",
    "            metric.add(l, acc, labels.shape[0], labels.numel())\n",
    "            timer.stop()\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (metric[0] / metric[2], metric[1] / metric[3],\n",
    "                              None))\n",
    "        test_acc = d2l.evaluate_accuracy_gpu(model, test_iter)\n",
    "        animator.add(epoch + 1, (None, None, test_acc))\n",
    "    print(f'loss {metric[0] / metric[2]:.3f}, train acc '\n",
    "          f'{metric[1] / metric[3]:.3f}, test acc {test_acc:.3f}')\n",
    "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on '\n",
    "          f'{str(devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "989e4710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CIFAR10_CNN()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model.to(device)\n",
    "# train(model, train_loader, test_loader, \n",
    "#       criterion, optimizer, num_epochs=15, devices=[device])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44459a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_simple_and_save(model, train_loader, test_loader,\n",
    "                          loss_fn, optimizer, num_epochs, device,\n",
    "                          save_dir=\"./logs\", id=\"test\", params=None):\n",
    "    \"\"\"\n",
    "    无动画训练并保存结果。\n",
    "    返回: 最终的 test_loss (float)\n",
    "    保存结构:\n",
    "      save_dir/\n",
    "        train_log.json  (训练参数、训练曲线、模型结构等)\n",
    "        figs/\n",
    "          loss.png\n",
    "          acc.png\n",
    "        model/\n",
    "          model.pth\n",
    "    params: 可选字典，记录超参等（会写入 train_log.json）\n",
    "    \"\"\"\n",
    "\n",
    "    save_base = Path(save_dir)\n",
    "    save_path = save_base / str(id)\n",
    "    figs_dir = save_path / \"figs\"\n",
    "    model_dir = save_path / \"model\"\n",
    "    figs_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    log = {\n",
    "        \"params\": params or {},\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": [],\n",
    "        \"model_structure\": str(model)\n",
    "    }\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for X, y in train_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_size = y.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            total_correct += (preds == y).sum().item()\n",
    "            total_samples += batch_size\n",
    "\n",
    "        train_loss = total_loss / max(1, total_samples)\n",
    "        train_acc = total_correct / max(1, total_samples)\n",
    "        log[\"train_loss\"].append(train_loss)\n",
    "        log[\"train_acc\"].append(train_acc)\n",
    "\n",
    "        # eval\n",
    "        model.eval()\n",
    "        t_loss = 0.0\n",
    "        t_correct = 0\n",
    "        t_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in test_loader:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                out = model(X)\n",
    "                l = loss_fn(out, y)\n",
    "                bs = y.size(0)\n",
    "                t_loss += l.item() * bs\n",
    "                t_correct += (out.argmax(dim=1) == y).sum().item()\n",
    "                t_samples += bs\n",
    "\n",
    "        test_loss = t_loss / max(1, t_samples)\n",
    "        test_acc = t_correct / max(1, t_samples)\n",
    "        log[\"test_loss\"].append(test_loss)\n",
    "        log[\"test_acc\"].append(test_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{num_epochs}  train_loss={train_loss:.4f}  test_loss={test_loss:.4f} | train_acc={train_acc:.4f}  test_acc={test_acc:.4f}\")\n",
    "\n",
    "    # # 保存曲线图\n",
    "    # plt.figure()\n",
    "    # plt.plot(range(1, num_epochs+1), log[\"train_loss\"], label=\"train_loss\")\n",
    "    # plt.plot(range(1, num_epochs+1), log[\"test_loss\"], label=\"test_loss\")\n",
    "    # plt.xlabel(\"epoch\")\n",
    "    # plt.ylabel(\"loss\")\n",
    "    # plt.legend()\n",
    "    # plt.grid(True)\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(figs_dir / \"loss.png\")\n",
    "    # plt.close()\n",
    "\n",
    "    # plt.figure()\n",
    "    # plt.plot(range(1, num_epochs+1), log[\"train_acc\"], label=\"train_acc\")\n",
    "    # plt.plot(range(1, num_epochs+1), log[\"test_acc\"], label=\"test_acc\")\n",
    "    # plt.xlabel(\"epoch\")\n",
    "    # plt.ylabel(\"acc\")\n",
    "    # plt.legend()\n",
    "    # plt.grid(True)\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(figs_dir / \"acc.png\")\n",
    "    # plt.close()\n",
    "\n",
    "    # # 保存模型 state_dict\n",
    "    # torch.save(model.state_dict(), model_dir / \"model.pth\")\n",
    "\n",
    "    # 准备可 JSON 序列化的日志并保存\n",
    "    json_log = {\n",
    "        \"params\": log[\"params\"],\n",
    "        \"train_loss\": [float(x) for x in log[\"train_loss\"]],\n",
    "        \"train_acc\": [float(x) for x in log[\"train_acc\"]],\n",
    "        \"test_loss\": [float(x) for x in log[\"test_loss\"]],\n",
    "        \"test_acc\": [float(x) for x in log[\"test_acc\"]],\n",
    "        \"model_structure\": log[\"model_structure\"],\n",
    "        \"final_test_loss\": float(log[\"test_loss\"][-1]) if log[\"test_loss\"] else None,\n",
    "        \"final_test_acc\": float(log[\"test_acc\"][-1]) if log[\"test_acc\"] else None\n",
    "    }\n",
    "    with open(save_path / \"train_log.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_log, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    return train_loss, json_log[\"final_test_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca06fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CIFAR10_CNN()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# params = {\"lr\":0.001, \"batch_size\":64, \"num_epochs\":15, \"model\":\"CIFAR10_CNN\"}\n",
    "# final_test_loss = train_simple_and_save(\n",
    "#     model, train_loader, test_loader,\n",
    "#     criterion, optimizer, num_epochs=20,\n",
    "#     device=device, save_dir=\"./logs\", id=\"test\", params=params)\n",
    "# print(\"final_test_loss =\", final_test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e4a87d",
   "metadata": {},
   "source": [
    "# 二、遗传算法实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5543129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from deap import base, creator, tools, algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bf199ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 定义适应度与个体类型（最小化 loss）\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(-1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b581d680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 各类基因生成器 (已简化为 7 项)\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# 基本超参数（个体字段顺序：batch_size, lr, dropout_rate, use_bn_cnn, use_dropout_cnn, use_bn_linear, use_dropout_linear）\n",
    "toolbox.register(\"batch_size\", lambda: random.choice([16, 32, 64]))  # 类别选择\n",
    "toolbox.register(\"lr\", lambda: 10**random.uniform(-5, -2))                  # 浮点（对数采样）\n",
    "toolbox.register(\"dropout_rate\", lambda: random.uniform(0.0, 0.7))      # 浮点（0.0-0.7）\n",
    "\n",
    "# cnn 超参数（已把层数、kernel、pooling 等固定为默认值以简化搜索）\n",
    "toolbox.register(\"use_bn_cnn\", lambda: random.choice([0,1]))                # 布尔（0/1）\n",
    "toolbox.register(\"use_dropout_cnn\", lambda: random.choice([0,1]))           # 布尔（0/1）\n",
    "\n",
    "# linear 超参数（层数固定，搜索是否使用 BN/Dropout）\n",
    "toolbox.register(\"use_bn_linear\", lambda: random.choice([0,1]))              # 布尔（0/1）\n",
    "toolbox.register(\"use_dropout_linear\", lambda: random.choice([0,1]))         # 布尔（0/1）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92af12cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 组合成个体（这里按顺序放入 list），简化为 7 项\n",
    "def create_individual():\n",
    "    # 结构： [batch_size, lr, dropout_rate, use_bn_cnn, use_dropout_cnn, use_bn_linear, use_dropout_linear]\n",
    "    return [\n",
    "        toolbox.batch_size(),\n",
    "        toolbox.lr(),\n",
    "        toolbox.dropout_rate(),\n",
    "        toolbox.use_bn_cnn(),\n",
    "        toolbox.use_dropout_cnn(),\n",
    "        toolbox.use_bn_linear(),\n",
    "        toolbox.use_dropout_linear()\n",
    "    ]\n",
    "\n",
    "toolbox.register(\"individual\", tools.initIterate, creator.Individual, create_individual)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70420097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于基因已简化，使用固定的默认结构参数，并从个体读取开关型超参\n",
    "def decode(ind):\n",
    "    # 对应 create_individual() 中的顺序\n",
    "    cfg = {\n",
    "        \"batch_size\": int(ind[0]),\n",
    "        \"lr\": float(ind[1]),\n",
    "        \"dropout_rate\": float(ind[2]),\n",
    "        # 固定的结构化超参数（为简化搜索）：\n",
    "        \"num_cnn_layers\": 3,\n",
    "        \"kernel_size\": 3,\n",
    "        \"pooling_type\": 'max',\n",
    "        \"num_linear_layers\": 2,\n",
    "        # 从基因读取的开关项\n",
    "        \"use_bn_cnn\": bool(int(ind[3])),\n",
    "        \"use_dropout_cnn\": bool(int(ind[4])),\n",
    "        \"use_bn_linear\": bool(int(ind[5])),\n",
    "        \"use_dropout_linear\": bool(int(ind[6]))\n",
    "    }\n",
    "    return cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a840a2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_from_cfg(cfg):\n",
    "    cnn_num_channels = [64, 128, 256, 512, 512][:cfg[\"num_cnn_layers\"]]\n",
    "    linear_hidden_size = [512, 256, 32, 16][:cfg[\"num_linear_layers\"]]\n",
    "    model = CIFAR10_CNN(\n",
    "        cnn_num_layers=cfg[\"num_cnn_layers\"],\n",
    "        cnn_num_channels=cnn_num_channels,\n",
    "        cnn_is_batch_norm=cfg[\"use_bn_cnn\"],\n",
    "        cnn_pooling_type=cfg[\"pooling_type\"],\n",
    "        cnn_is_dropout=cfg[\"use_dropout_cnn\"],\n",
    "        cnn_dropout_rate=cfg[\"dropout_rate\"],\n",
    "        linear_num_layers=cfg[\"num_linear_layers\"],\n",
    "        linear_hidden_size=linear_hidden_size,\n",
    "        linear_is_batch_norm=cfg[\"use_bn_linear\"],\n",
    "        linear_is_dropout=cfg[\"use_dropout_linear\"],\n",
    "        linear_dropout_rate=cfg[\"dropout_rate\"]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3b25d1",
   "metadata": {},
   "source": [
    "    # 对应 create_individual() 中的顺序\n",
    "    cfg = {\n",
    "        \"batch_size\": int(ind[0]),\n",
    "        \"lr\": float(ind[1]),\n",
    "        \"dropout_rate\": float(ind[2]),\n",
    "        # 固定的结构化超参数（为简化搜索）：\n",
    "        \"num_cnn_layers\": 3,\n",
    "        \"kernel_size\": 3,\n",
    "        \"pooling_type\": 'max',\n",
    "        \"num_linear_layers\": 2,\n",
    "        # 从基因读取的开关项\n",
    "        \"use_bn_cnn\": bool(int(ind[3])),\n",
    "        \"use_dropout_cnn\": bool(int(ind[4])),\n",
    "        \"use_bn_linear\": bool(int(ind[5])),\n",
    "        \"use_dropout_linear\": bool(int(ind[6]))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9794a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        cfg, num_epochs=50, id=\"temp\", seed=None, \n",
    "        sample_count_train=2000, sample_count_test=500):\n",
    "    # 支持传入 seed 和可选的子集大小，方便确定性评估和快速试验\n",
    "    train_loader = load_cifar10(\n",
    "        is_train=True,\n",
    "        augs=transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.ToTensor()]),\n",
    "        batch_size=cfg[\"batch_size\"],\n",
    "        sample_count=sample_count_train,\n",
    "        seed=seed)\n",
    "    test_loader = load_cifar10(\n",
    "        is_train=False,\n",
    "        augs=transforms.Compose([transforms.ToTensor()]),\n",
    "        batch_size=cfg[\"batch_size\"],\n",
    "        sample_count=sample_count_test,\n",
    "        seed=seed)\n",
    "    model = create_model_from_cfg(cfg)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg[\"lr\"])\n",
    "    \n",
    "    # 输出参数\n",
    "    print(f'num_epochs={num_epochs}, sample_count={sample_count_train}')\n",
    "    print(f'{cfg[\"batch_size\"]}, {cfg[\"lr\"]}, {cfg[\"dropout_rate\"]}')\n",
    "    print(f'{cfg[\"use_bn_cnn\"]}, {cfg[\"use_dropout_cnn\"]}')\n",
    "    print(f'{cfg[\"use_bn_linear\"]}, {cfg[\"use_dropout_linear\"]}')\n",
    "\n",
    "    train_loss, test_loss = train_simple_and_save(\n",
    "        model, train_loader, test_loader,\n",
    "        criterion, optimizer, num_epochs,\n",
    "        device=device, save_dir=\"./temp\", id=id, params=cfg)\n",
    "    \n",
    "    return test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03805eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate_individual(ind, indpb=0.1):\n",
    "    # 按位以 indpb 概率用对应 generator 重新采样该位（适用于异构基因）\n",
    "    for i in range(len(ind)):\n",
    "        if random.random() >= indpb:\n",
    "            continue\n",
    "        if i == 0:\n",
    "            ind[i] = toolbox.batch_size()\n",
    "        elif i == 1:\n",
    "            ind[i] = toolbox.lr()\n",
    "        elif i == 2:\n",
    "            ind[i] = toolbox.dropout_rate()\n",
    "        elif i == 3:\n",
    "            ind[i] = toolbox.use_bn_cnn()\n",
    "        elif i == 4:\n",
    "            ind[i] = toolbox.use_dropout_cnn()\n",
    "        elif i == 5:\n",
    "            ind[i] = toolbox.use_bn_linear()\n",
    "        elif i == 6:\n",
    "            ind[i] = toolbox.use_dropout_linear()\n",
    "    return ind,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7412a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 示例评估函数（应返回 tuple）\n",
    "def evaluate(ind):\n",
    "    cfg = decode(ind)\n",
    "    # 基于个体内容生成确定性 seed，减少评估方差\n",
    "    seed = int(abs(hash(tuple(ind))) % (2**32))\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # 训练模型（将 seed 传递给数据加载以保证子集选择一致，如果使用子集的话）\n",
    "    val_loss = train_model(cfg, num_epochs=15, id=\"temp\", seed=seed)\n",
    "    # 确保返回 Python float\n",
    "    return (float(val_loss),)\n",
    "\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", mutate_individual, indpb=0.1)  # 根据基因类型可自定义\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "553cb302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epochs=15, sample_count=2000\n",
      "64, 2.2375361238944334e-05, 0.5053317712158663\n",
      "False, True\n",
      "False, True\n",
      "Epoch 1/15  train_loss=2.3033  test_loss=2.3026 | train_acc=0.1065  test_acc=0.1000\n",
      "Epoch 1/15  train_loss=2.3033  test_loss=2.3026 | train_acc=0.1065  test_acc=0.1000\n",
      "Epoch 2/15  train_loss=2.2951  test_loss=2.3016 | train_acc=0.1455  test_acc=0.1000\n",
      "Epoch 2/15  train_loss=2.2951  test_loss=2.3016 | train_acc=0.1455  test_acc=0.1000\n",
      "Epoch 3/15  train_loss=2.2935  test_loss=2.3004 | train_acc=0.1170  test_acc=0.1412\n",
      "Epoch 3/15  train_loss=2.2935  test_loss=2.3004 | train_acc=0.1170  test_acc=0.1412\n",
      "Epoch 4/15  train_loss=2.2865  test_loss=2.2991 | train_acc=0.1285  test_acc=0.1454\n",
      "Epoch 4/15  train_loss=2.2865  test_loss=2.2991 | train_acc=0.1285  test_acc=0.1454\n",
      "Epoch 5/15  train_loss=2.2773  test_loss=2.2967 | train_acc=0.1280  test_acc=0.1629\n",
      "Epoch 5/15  train_loss=2.2773  test_loss=2.2967 | train_acc=0.1280  test_acc=0.1629\n",
      "Epoch 6/15  train_loss=2.2645  test_loss=2.2932 | train_acc=0.1470  test_acc=0.1786\n",
      "Epoch 6/15  train_loss=2.2645  test_loss=2.2932 | train_acc=0.1470  test_acc=0.1786\n",
      "Epoch 7/15  train_loss=2.2509  test_loss=2.2877 | train_acc=0.1595  test_acc=0.2247\n",
      "Epoch 7/15  train_loss=2.2509  test_loss=2.2877 | train_acc=0.1595  test_acc=0.2247\n",
      "Epoch 8/15  train_loss=2.2227  test_loss=2.2785 | train_acc=0.1845  test_acc=0.2123\n",
      "Epoch 8/15  train_loss=2.2227  test_loss=2.2785 | train_acc=0.1845  test_acc=0.2123\n",
      "Epoch 9/15  train_loss=2.1972  test_loss=2.2688 | train_acc=0.1860  test_acc=0.2182\n",
      "Epoch 9/15  train_loss=2.1972  test_loss=2.2688 | train_acc=0.1860  test_acc=0.2182\n",
      "Epoch 10/15  train_loss=2.1662  test_loss=2.2542 | train_acc=0.2205  test_acc=0.2245\n",
      "Epoch 10/15  train_loss=2.1662  test_loss=2.2542 | train_acc=0.2205  test_acc=0.2245\n",
      "Epoch 11/15  train_loss=2.1279  test_loss=2.2436 | train_acc=0.2270  test_acc=0.2259\n",
      "Epoch 11/15  train_loss=2.1279  test_loss=2.2436 | train_acc=0.2270  test_acc=0.2259\n",
      "Epoch 12/15  train_loss=2.0969  test_loss=2.2360 | train_acc=0.2155  test_acc=0.2163\n",
      "Epoch 12/15  train_loss=2.0969  test_loss=2.2360 | train_acc=0.2155  test_acc=0.2163\n",
      "Epoch 13/15  train_loss=2.0942  test_loss=2.2289 | train_acc=0.2160  test_acc=0.2268\n",
      "Epoch 13/15  train_loss=2.0942  test_loss=2.2289 | train_acc=0.2160  test_acc=0.2268\n",
      "Epoch 14/15  train_loss=2.0805  test_loss=2.2247 | train_acc=0.2320  test_acc=0.2302\n",
      "Epoch 14/15  train_loss=2.0805  test_loss=2.2247 | train_acc=0.2320  test_acc=0.2302\n",
      "Epoch 15/15  train_loss=2.0718  test_loss=2.2177 | train_acc=0.2190  test_acc=0.2296\n",
      "Epoch 15/15  train_loss=2.0718  test_loss=2.2177 | train_acc=0.2190  test_acc=0.2296\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "num_epochs=15, sample_count=2000\n",
      "64, 0.003382591335291819, 0.6125839669478541\n",
      "True, True\n",
      "True, False\n",
      "num_epochs=15, sample_count=2000\n",
      "64, 0.003382591335291819, 0.6125839669478541\n",
      "True, True\n",
      "True, False\n",
      "Epoch 1/15  train_loss=2.0378  test_loss=3.1311 | train_acc=0.2600  test_acc=0.1001\n",
      "Epoch 1/15  train_loss=2.0378  test_loss=3.1311 | train_acc=0.2600  test_acc=0.1001\n",
      "Epoch 2/15  train_loss=1.8073  test_loss=3.0790 | train_acc=0.3350  test_acc=0.1209\n",
      "Epoch 2/15  train_loss=1.8073  test_loss=3.0790 | train_acc=0.3350  test_acc=0.1209\n",
      "Epoch 3/15  train_loss=1.6966  test_loss=2.9088 | train_acc=0.3600  test_acc=0.1634\n",
      "Epoch 3/15  train_loss=1.6966  test_loss=2.9088 | train_acc=0.3600  test_acc=0.1634\n",
      "Epoch 4/15  train_loss=1.6342  test_loss=2.6637 | train_acc=0.3960  test_acc=0.2539\n",
      "Epoch 4/15  train_loss=1.6342  test_loss=2.6637 | train_acc=0.3960  test_acc=0.2539\n",
      "Epoch 5/15  train_loss=1.6039  test_loss=3.1775 | train_acc=0.4000  test_acc=0.1174\n",
      "Epoch 5/15  train_loss=1.6039  test_loss=3.1775 | train_acc=0.4000  test_acc=0.1174\n",
      "Epoch 6/15  train_loss=1.5809  test_loss=3.1694 | train_acc=0.4070  test_acc=0.1079\n",
      "Epoch 6/15  train_loss=1.5809  test_loss=3.1694 | train_acc=0.4070  test_acc=0.1079\n",
      "Epoch 7/15  train_loss=1.5371  test_loss=2.7713 | train_acc=0.4345  test_acc=0.2098\n",
      "Epoch 7/15  train_loss=1.5371  test_loss=2.7713 | train_acc=0.4345  test_acc=0.2098\n",
      "Epoch 8/15  train_loss=1.5177  test_loss=3.0579 | train_acc=0.4435  test_acc=0.1075\n",
      "Epoch 8/15  train_loss=1.5177  test_loss=3.0579 | train_acc=0.4435  test_acc=0.1075\n",
      "Epoch 9/15  train_loss=1.4541  test_loss=3.0270 | train_acc=0.4590  test_acc=0.1114\n",
      "Epoch 9/15  train_loss=1.4541  test_loss=3.0270 | train_acc=0.4590  test_acc=0.1114\n",
      "Epoch 10/15  train_loss=1.4431  test_loss=3.6079 | train_acc=0.4490  test_acc=0.1005\n",
      "Epoch 10/15  train_loss=1.4431  test_loss=3.6079 | train_acc=0.4490  test_acc=0.1005\n",
      "Epoch 11/15  train_loss=1.3957  test_loss=2.7995 | train_acc=0.4830  test_acc=0.1348\n",
      "Epoch 11/15  train_loss=1.3957  test_loss=2.7995 | train_acc=0.4830  test_acc=0.1348\n",
      "Epoch 12/15  train_loss=1.3358  test_loss=3.6690 | train_acc=0.5070  test_acc=0.1169\n",
      "Epoch 12/15  train_loss=1.3358  test_loss=3.6690 | train_acc=0.5070  test_acc=0.1169\n",
      "Epoch 13/15  train_loss=1.3512  test_loss=3.6120 | train_acc=0.4865  test_acc=0.1085\n",
      "Epoch 13/15  train_loss=1.3512  test_loss=3.6120 | train_acc=0.4865  test_acc=0.1085\n",
      "Epoch 14/15  train_loss=1.3158  test_loss=3.2720 | train_acc=0.5075  test_acc=0.1299\n",
      "Epoch 14/15  train_loss=1.3158  test_loss=3.2720 | train_acc=0.5075  test_acc=0.1299\n",
      "Epoch 15/15  train_loss=1.3132  test_loss=3.4028 | train_acc=0.5085  test_acc=0.1075\n",
      "Epoch 15/15  train_loss=1.3132  test_loss=3.4028 | train_acc=0.5085  test_acc=0.1075\n",
      "num_epochs=15, sample_count=2000\n",
      "64, 0.00789975198202792, 0.5482891751179184\n",
      "False, False\n",
      "False, False\n",
      "num_epochs=15, sample_count=2000\n",
      "64, 0.00789975198202792, 0.5482891751179184\n",
      "False, False\n",
      "False, False\n",
      "Epoch 1/15  train_loss=2.4715  test_loss=2.2969 | train_acc=0.1105  test_acc=0.1150\n",
      "Epoch 1/15  train_loss=2.4715  test_loss=2.2969 | train_acc=0.1105  test_acc=0.1150\n",
      "Epoch 2/15  train_loss=2.2306  test_loss=2.1043 | train_acc=0.1700  test_acc=0.2313\n",
      "Epoch 2/15  train_loss=2.2306  test_loss=2.1043 | train_acc=0.1700  test_acc=0.2313\n",
      "Epoch 3/15  train_loss=2.1218  test_loss=2.0391 | train_acc=0.1960  test_acc=0.2435\n",
      "Epoch 3/15  train_loss=2.1218  test_loss=2.0391 | train_acc=0.1960  test_acc=0.2435\n",
      "Epoch 4/15  train_loss=2.0305  test_loss=1.9914 | train_acc=0.2210  test_acc=0.2409\n",
      "Epoch 4/15  train_loss=2.0305  test_loss=1.9914 | train_acc=0.2210  test_acc=0.2409\n",
      "Epoch 5/15  train_loss=2.0298  test_loss=2.0033 | train_acc=0.2230  test_acc=0.2533\n",
      "Epoch 5/15  train_loss=2.0298  test_loss=2.0033 | train_acc=0.2230  test_acc=0.2533\n",
      "Epoch 6/15  train_loss=1.9735  test_loss=1.9432 | train_acc=0.2425  test_acc=0.2719\n",
      "Epoch 6/15  train_loss=1.9735  test_loss=1.9432 | train_acc=0.2425  test_acc=0.2719\n",
      "Epoch 7/15  train_loss=1.9255  test_loss=1.9907 | train_acc=0.2795  test_acc=0.2649\n",
      "Epoch 7/15  train_loss=1.9255  test_loss=1.9907 | train_acc=0.2795  test_acc=0.2649\n",
      "Epoch 8/15  train_loss=1.9104  test_loss=1.8778 | train_acc=0.2885  test_acc=0.3122\n",
      "Epoch 8/15  train_loss=1.9104  test_loss=1.8778 | train_acc=0.2885  test_acc=0.3122\n",
      "Epoch 9/15  train_loss=1.8586  test_loss=1.8139 | train_acc=0.3145  test_acc=0.3217\n",
      "Epoch 9/15  train_loss=1.8586  test_loss=1.8139 | train_acc=0.3145  test_acc=0.3217\n",
      "Epoch 10/15  train_loss=1.8194  test_loss=1.8245 | train_acc=0.3180  test_acc=0.3386\n",
      "Epoch 10/15  train_loss=1.8194  test_loss=1.8245 | train_acc=0.3180  test_acc=0.3386\n",
      "Epoch 11/15  train_loss=1.8207  test_loss=1.8305 | train_acc=0.3230  test_acc=0.3242\n",
      "Epoch 11/15  train_loss=1.8207  test_loss=1.8305 | train_acc=0.3230  test_acc=0.3242\n",
      "Epoch 12/15  train_loss=1.7682  test_loss=1.8016 | train_acc=0.3385  test_acc=0.3216\n",
      "Epoch 12/15  train_loss=1.7682  test_loss=1.8016 | train_acc=0.3385  test_acc=0.3216\n",
      "Epoch 13/15  train_loss=1.7586  test_loss=1.7476 | train_acc=0.3495  test_acc=0.3626\n",
      "Epoch 13/15  train_loss=1.7586  test_loss=1.7476 | train_acc=0.3495  test_acc=0.3626\n",
      "Epoch 14/15  train_loss=1.7298  test_loss=1.7614 | train_acc=0.3485  test_acc=0.3529\n",
      "Epoch 14/15  train_loss=1.7298  test_loss=1.7614 | train_acc=0.3485  test_acc=0.3529\n",
      "Epoch 15/15  train_loss=1.7409  test_loss=1.8060 | train_acc=0.3545  test_acc=0.3279\n",
      "Epoch 15/15  train_loss=1.7409  test_loss=1.8060 | train_acc=0.3545  test_acc=0.3279\n",
      "num_epochs=15, sample_count=2000\n",
      "64, 0.002759192797298154, 0.07762888170300866\n",
      "False, True\n",
      "True, True\n",
      "num_epochs=15, sample_count=2000\n",
      "64, 0.002759192797298154, 0.07762888170300866\n",
      "False, True\n",
      "True, True\n",
      "Epoch 1/15  train_loss=2.2641  test_loss=2.7312 | train_acc=0.1885  test_acc=0.1186\n",
      "Epoch 1/15  train_loss=2.2641  test_loss=2.7312 | train_acc=0.1885  test_acc=0.1186\n",
      "Epoch 2/15  train_loss=2.0211  test_loss=1.9060 | train_acc=0.2605  test_acc=0.3015\n",
      "Epoch 2/15  train_loss=2.0211  test_loss=1.9060 | train_acc=0.2605  test_acc=0.3015\n",
      "Epoch 3/15  train_loss=1.8940  test_loss=2.0080 | train_acc=0.3040  test_acc=0.2804\n",
      "Epoch 3/15  train_loss=1.8940  test_loss=2.0080 | train_acc=0.3040  test_acc=0.2804\n",
      "Epoch 4/15  train_loss=1.8468  test_loss=2.9928 | train_acc=0.3175  test_acc=0.2204\n",
      "Epoch 4/15  train_loss=1.8468  test_loss=2.9928 | train_acc=0.3175  test_acc=0.2204\n",
      "Epoch 5/15  train_loss=1.7185  test_loss=3.5636 | train_acc=0.3730  test_acc=0.1490\n",
      "Epoch 5/15  train_loss=1.7185  test_loss=3.5636 | train_acc=0.3730  test_acc=0.1490\n",
      "Epoch 6/15  train_loss=1.6652  test_loss=3.4956 | train_acc=0.3835  test_acc=0.1414\n",
      "Epoch 6/15  train_loss=1.6652  test_loss=3.4956 | train_acc=0.3835  test_acc=0.1414\n",
      "Epoch 7/15  train_loss=1.6587  test_loss=3.1116 | train_acc=0.3970  test_acc=0.2179\n",
      "Epoch 7/15  train_loss=1.6587  test_loss=3.1116 | train_acc=0.3970  test_acc=0.2179\n",
      "Epoch 8/15  train_loss=1.6195  test_loss=2.9614 | train_acc=0.4040  test_acc=0.2279\n",
      "Epoch 8/15  train_loss=1.6195  test_loss=2.9614 | train_acc=0.4040  test_acc=0.2279\n",
      "Epoch 9/15  train_loss=1.5803  test_loss=2.7646 | train_acc=0.4160  test_acc=0.2582\n",
      "Epoch 9/15  train_loss=1.5803  test_loss=2.7646 | train_acc=0.4160  test_acc=0.2582\n",
      "Epoch 10/15  train_loss=1.5554  test_loss=1.6509 | train_acc=0.4370  test_acc=0.4133\n",
      "Epoch 10/15  train_loss=1.5554  test_loss=1.6509 | train_acc=0.4370  test_acc=0.4133\n",
      "Epoch 11/15  train_loss=1.5201  test_loss=2.2055 | train_acc=0.4420  test_acc=0.3108\n",
      "Epoch 11/15  train_loss=1.5201  test_loss=2.2055 | train_acc=0.4420  test_acc=0.3108\n",
      "Epoch 12/15  train_loss=1.4973  test_loss=2.2776 | train_acc=0.4680  test_acc=0.3086\n",
      "Epoch 12/15  train_loss=1.4973  test_loss=2.2776 | train_acc=0.4680  test_acc=0.3086\n",
      "Epoch 13/15  train_loss=1.4577  test_loss=2.1526 | train_acc=0.4770  test_acc=0.3218\n",
      "Epoch 13/15  train_loss=1.4577  test_loss=2.1526 | train_acc=0.4770  test_acc=0.3218\n",
      "Epoch 14/15  train_loss=1.4258  test_loss=1.9001 | train_acc=0.4840  test_acc=0.3704\n",
      "Epoch 14/15  train_loss=1.4258  test_loss=1.9001 | train_acc=0.4840  test_acc=0.3704\n",
      "Epoch 15/15  train_loss=1.4075  test_loss=2.7111 | train_acc=0.4945  test_acc=0.2335\n",
      "gen\tnevals\tavg    \tmin    \tmax    \n",
      "0  \t5     \t2.26272\t1.17598\t3.40284\n",
      "Epoch 15/15  train_loss=1.4075  test_loss=2.7111 | train_acc=0.4945  test_acc=0.2335\n",
      "gen\tnevals\tavg    \tmin    \tmax    \n",
      "0  \t5     \t2.26272\t1.17598\t3.40284\n",
      "num_epochs=15, sample_count=2000\n",
      "64, 2.2375361238944334e-05, 0.5053317712158663\n",
      "False, True\n",
      "False, True\n",
      "num_epochs=15, sample_count=2000\n",
      "64, 2.2375361238944334e-05, 0.5053317712158663\n",
      "False, True\n",
      "False, True\n",
      "Epoch 1/15  train_loss=2.3033  test_loss=2.3026 | train_acc=0.1065  test_acc=0.1000\n",
      "Epoch 1/15  train_loss=2.3033  test_loss=2.3026 | train_acc=0.1065  test_acc=0.1000\n",
      "Epoch 2/15  train_loss=2.2951  test_loss=2.3016 | train_acc=0.1455  test_acc=0.1000\n",
      "Epoch 2/15  train_loss=2.2951  test_loss=2.3016 | train_acc=0.1455  test_acc=0.1000\n",
      "Epoch 3/15  train_loss=2.2935  test_loss=2.3004 | train_acc=0.1170  test_acc=0.1412\n",
      "Epoch 3/15  train_loss=2.2935  test_loss=2.3004 | train_acc=0.1170  test_acc=0.1412\n",
      "Epoch 4/15  train_loss=2.2865  test_loss=2.2991 | train_acc=0.1285  test_acc=0.1454\n",
      "Epoch 4/15  train_loss=2.2865  test_loss=2.2991 | train_acc=0.1285  test_acc=0.1454\n",
      "Epoch 5/15  train_loss=2.2773  test_loss=2.2967 | train_acc=0.1280  test_acc=0.1629\n",
      "Epoch 5/15  train_loss=2.2773  test_loss=2.2967 | train_acc=0.1280  test_acc=0.1629\n",
      "Epoch 6/15  train_loss=2.2645  test_loss=2.2932 | train_acc=0.1470  test_acc=0.1786\n",
      "Epoch 6/15  train_loss=2.2645  test_loss=2.2932 | train_acc=0.1470  test_acc=0.1786\n",
      "Epoch 7/15  train_loss=2.2509  test_loss=2.2877 | train_acc=0.1595  test_acc=0.2247\n",
      "Epoch 7/15  train_loss=2.2509  test_loss=2.2877 | train_acc=0.1595  test_acc=0.2247\n",
      "Epoch 8/15  train_loss=2.2227  test_loss=2.2785 | train_acc=0.1845  test_acc=0.2123\n",
      "Epoch 8/15  train_loss=2.2227  test_loss=2.2785 | train_acc=0.1845  test_acc=0.2123\n",
      "Epoch 9/15  train_loss=2.1972  test_loss=2.2688 | train_acc=0.1860  test_acc=0.2182\n",
      "Epoch 9/15  train_loss=2.1972  test_loss=2.2688 | train_acc=0.1860  test_acc=0.2182\n",
      "Epoch 10/15  train_loss=2.1662  test_loss=2.2542 | train_acc=0.2205  test_acc=0.2245\n",
      "Epoch 10/15  train_loss=2.1662  test_loss=2.2542 | train_acc=0.2205  test_acc=0.2245\n",
      "Epoch 11/15  train_loss=2.1279  test_loss=2.2436 | train_acc=0.2270  test_acc=0.2259\n",
      "Epoch 11/15  train_loss=2.1279  test_loss=2.2436 | train_acc=0.2270  test_acc=0.2259\n",
      "Epoch 12/15  train_loss=2.0969  test_loss=2.2360 | train_acc=0.2155  test_acc=0.2163\n",
      "Epoch 12/15  train_loss=2.0969  test_loss=2.2360 | train_acc=0.2155  test_acc=0.2163\n",
      "Epoch 13/15  train_loss=2.0942  test_loss=2.2289 | train_acc=0.2160  test_acc=0.2268\n",
      "Epoch 13/15  train_loss=2.0942  test_loss=2.2289 | train_acc=0.2160  test_acc=0.2268\n",
      "Epoch 14/15  train_loss=2.0805  test_loss=2.2247 | train_acc=0.2320  test_acc=0.2302\n",
      "Epoch 14/15  train_loss=2.0805  test_loss=2.2247 | train_acc=0.2320  test_acc=0.2302\n",
      "Epoch 15/15  train_loss=2.0718  test_loss=2.2177 | train_acc=0.2190  test_acc=0.2296\n",
      "Epoch 15/15  train_loss=2.0718  test_loss=2.2177 | train_acc=0.2190  test_acc=0.2296\n",
      "num_epochs=15, sample_count=2000\n",
      "64, 0.00789975198202792, 0.5482891751179184\n",
      "False, False\n",
      "True, False\n",
      "num_epochs=15, sample_count=2000\n",
      "64, 0.00789975198202792, 0.5482891751179184\n",
      "False, False\n",
      "True, False\n",
      "Epoch 1/15  train_loss=2.3154  test_loss=2.3076 | train_acc=0.1765  test_acc=0.1979\n",
      "Epoch 1/15  train_loss=2.3154  test_loss=2.3076 | train_acc=0.1765  test_acc=0.1979\n",
      "Epoch 2/15  train_loss=2.1114  test_loss=2.1964 | train_acc=0.2175  test_acc=0.2379\n",
      "Epoch 2/15  train_loss=2.1114  test_loss=2.1964 | train_acc=0.2175  test_acc=0.2379\n",
      "Epoch 3/15  train_loss=2.0776  test_loss=2.3332 | train_acc=0.2715  test_acc=0.1932\n",
      "Epoch 3/15  train_loss=2.0776  test_loss=2.3332 | train_acc=0.2715  test_acc=0.1932\n",
      "Epoch 4/15  train_loss=1.9060  test_loss=2.6677 | train_acc=0.3160  test_acc=0.2004\n",
      "Epoch 4/15  train_loss=1.9060  test_loss=2.6677 | train_acc=0.3160  test_acc=0.2004\n",
      "Epoch 5/15  train_loss=1.8927  test_loss=2.1973 | train_acc=0.3200  test_acc=0.2406\n",
      "Epoch 5/15  train_loss=1.8927  test_loss=2.1973 | train_acc=0.3200  test_acc=0.2406\n",
      "Epoch 6/15  train_loss=1.7752  test_loss=376.9323 | train_acc=0.3435  test_acc=0.1000\n",
      "Epoch 6/15  train_loss=1.7752  test_loss=376.9323 | train_acc=0.3435  test_acc=0.1000\n",
      "Epoch 7/15  train_loss=1.7240  test_loss=2.1359 | train_acc=0.3605  test_acc=0.3151\n",
      "Epoch 7/15  train_loss=1.7240  test_loss=2.1359 | train_acc=0.3605  test_acc=0.3151\n",
      "Epoch 8/15  train_loss=1.6649  test_loss=2.1696 | train_acc=0.3980  test_acc=0.3447\n",
      "Epoch 8/15  train_loss=1.6649  test_loss=2.1696 | train_acc=0.3980  test_acc=0.3447\n",
      "Epoch 9/15  train_loss=1.5963  test_loss=1.8964 | train_acc=0.4260  test_acc=0.3507\n",
      "Epoch 9/15  train_loss=1.5963  test_loss=1.8964 | train_acc=0.4260  test_acc=0.3507\n",
      "Epoch 10/15  train_loss=1.6005  test_loss=6.5685 | train_acc=0.4120  test_acc=0.2799\n",
      "Epoch 10/15  train_loss=1.6005  test_loss=6.5685 | train_acc=0.4120  test_acc=0.2799\n",
      "Epoch 11/15  train_loss=1.5655  test_loss=3.7415 | train_acc=0.4220  test_acc=0.3962\n",
      "Epoch 11/15  train_loss=1.5655  test_loss=3.7415 | train_acc=0.4220  test_acc=0.3962\n",
      "Epoch 12/15  train_loss=1.5387  test_loss=43.7402 | train_acc=0.4550  test_acc=0.4075\n",
      "Epoch 12/15  train_loss=1.5387  test_loss=43.7402 | train_acc=0.4550  test_acc=0.4075\n",
      "Epoch 13/15  train_loss=1.5033  test_loss=1000.5127 | train_acc=0.4655  test_acc=0.3097\n",
      "Epoch 13/15  train_loss=1.5033  test_loss=1000.5127 | train_acc=0.4655  test_acc=0.3097\n",
      "Epoch 14/15  train_loss=1.4996  test_loss=80.3998 | train_acc=0.4550  test_acc=0.3797\n",
      "Epoch 14/15  train_loss=1.4996  test_loss=80.3998 | train_acc=0.4550  test_acc=0.3797\n",
      "Epoch 15/15  train_loss=1.4324  test_loss=27.8358 | train_acc=0.4815  test_acc=0.4652\n",
      "Epoch 15/15  train_loss=1.4324  test_loss=27.8358 | train_acc=0.4815  test_acc=0.4652\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "False, False\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "False, False\n",
      "Epoch 1/15  train_loss=2.0672  test_loss=1.8084 | train_acc=0.2275  test_acc=0.3429\n",
      "Epoch 1/15  train_loss=2.0672  test_loss=1.8084 | train_acc=0.2275  test_acc=0.3429\n",
      "Epoch 2/15  train_loss=1.7664  test_loss=1.6292 | train_acc=0.3640  test_acc=0.3910\n",
      "Epoch 2/15  train_loss=1.7664  test_loss=1.6292 | train_acc=0.3640  test_acc=0.3910\n",
      "Epoch 3/15  train_loss=1.6571  test_loss=1.5422 | train_acc=0.3785  test_acc=0.4300\n",
      "Epoch 3/15  train_loss=1.6571  test_loss=1.5422 | train_acc=0.3785  test_acc=0.4300\n",
      "Epoch 4/15  train_loss=1.5842  test_loss=1.5289 | train_acc=0.4200  test_acc=0.4163\n",
      "Epoch 4/15  train_loss=1.5842  test_loss=1.5289 | train_acc=0.4200  test_acc=0.4163\n",
      "Epoch 5/15  train_loss=1.5341  test_loss=1.4688 | train_acc=0.4360  test_acc=0.4653\n",
      "Epoch 5/15  train_loss=1.5341  test_loss=1.4688 | train_acc=0.4360  test_acc=0.4653\n",
      "Epoch 6/15  train_loss=1.4630  test_loss=1.4322 | train_acc=0.4710  test_acc=0.4709\n",
      "Epoch 6/15  train_loss=1.4630  test_loss=1.4322 | train_acc=0.4710  test_acc=0.4709\n",
      "Epoch 7/15  train_loss=1.4129  test_loss=1.4369 | train_acc=0.4915  test_acc=0.4801\n",
      "Epoch 7/15  train_loss=1.4129  test_loss=1.4369 | train_acc=0.4915  test_acc=0.4801\n",
      "Epoch 8/15  train_loss=1.3716  test_loss=1.4479 | train_acc=0.5010  test_acc=0.4637\n",
      "Epoch 8/15  train_loss=1.3716  test_loss=1.4479 | train_acc=0.5010  test_acc=0.4637\n",
      "Epoch 9/15  train_loss=1.3330  test_loss=1.3031 | train_acc=0.5195  test_acc=0.5212\n",
      "Epoch 9/15  train_loss=1.3330  test_loss=1.3031 | train_acc=0.5195  test_acc=0.5212\n",
      "Epoch 10/15  train_loss=1.2926  test_loss=1.3087 | train_acc=0.5400  test_acc=0.5161\n",
      "Epoch 10/15  train_loss=1.2926  test_loss=1.3087 | train_acc=0.5400  test_acc=0.5161\n",
      "Epoch 11/15  train_loss=1.2578  test_loss=1.3192 | train_acc=0.5430  test_acc=0.5146\n",
      "Epoch 11/15  train_loss=1.2578  test_loss=1.3192 | train_acc=0.5430  test_acc=0.5146\n",
      "Epoch 12/15  train_loss=1.2138  test_loss=1.2984 | train_acc=0.5655  test_acc=0.5387\n",
      "Epoch 12/15  train_loss=1.2138  test_loss=1.2984 | train_acc=0.5655  test_acc=0.5387\n",
      "Epoch 13/15  train_loss=1.1783  test_loss=1.2377 | train_acc=0.5770  test_acc=0.5548\n",
      "Epoch 13/15  train_loss=1.1783  test_loss=1.2377 | train_acc=0.5770  test_acc=0.5548\n",
      "Epoch 14/15  train_loss=1.1458  test_loss=1.1887 | train_acc=0.5870  test_acc=0.5681\n",
      "Epoch 14/15  train_loss=1.1458  test_loss=1.1887 | train_acc=0.5870  test_acc=0.5681\n",
      "Epoch 15/15  train_loss=1.1030  test_loss=1.1784 | train_acc=0.6075  test_acc=0.5706\n",
      "1  \t3     \t6.71678\t1.17598\t27.8358\n",
      "Epoch 15/15  train_loss=1.1030  test_loss=1.1784 | train_acc=0.6075  test_acc=0.5706\n",
      "1  \t3     \t6.71678\t1.17598\t27.8358\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 2.2375361238944334e-05, 0.5053317712158663\n",
      "False, True\n",
      "False, True\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 2.2375361238944334e-05, 0.5053317712158663\n",
      "False, True\n",
      "False, True\n",
      "Epoch 1/15  train_loss=2.3003  test_loss=2.3014 | train_acc=0.1060  test_acc=0.1003\n",
      "Epoch 1/15  train_loss=2.3003  test_loss=2.3014 | train_acc=0.1060  test_acc=0.1003\n",
      "Epoch 2/15  train_loss=2.2905  test_loss=2.2981 | train_acc=0.1280  test_acc=0.1150\n",
      "Epoch 2/15  train_loss=2.2905  test_loss=2.2981 | train_acc=0.1280  test_acc=0.1150\n",
      "Epoch 3/15  train_loss=2.2654  test_loss=2.2861 | train_acc=0.1555  test_acc=0.1944\n",
      "Epoch 3/15  train_loss=2.2654  test_loss=2.2861 | train_acc=0.1555  test_acc=0.1944\n",
      "Epoch 4/15  train_loss=2.1866  test_loss=2.2543 | train_acc=0.1985  test_acc=0.2028\n",
      "Epoch 4/15  train_loss=2.1866  test_loss=2.2543 | train_acc=0.1985  test_acc=0.2028\n",
      "Epoch 5/15  train_loss=2.1407  test_loss=2.2359 | train_acc=0.2015  test_acc=0.2174\n",
      "Epoch 5/15  train_loss=2.1407  test_loss=2.2359 | train_acc=0.2015  test_acc=0.2174\n",
      "Epoch 6/15  train_loss=2.1098  test_loss=2.2305 | train_acc=0.2090  test_acc=0.2209\n",
      "Epoch 6/15  train_loss=2.1098  test_loss=2.2305 | train_acc=0.2090  test_acc=0.2209\n",
      "Epoch 7/15  train_loss=2.0933  test_loss=2.2271 | train_acc=0.2090  test_acc=0.2282\n",
      "Epoch 7/15  train_loss=2.0933  test_loss=2.2271 | train_acc=0.2090  test_acc=0.2282\n",
      "Epoch 8/15  train_loss=2.0763  test_loss=2.2161 | train_acc=0.2300  test_acc=0.2385\n",
      "Epoch 8/15  train_loss=2.0763  test_loss=2.2161 | train_acc=0.2300  test_acc=0.2385\n",
      "Epoch 9/15  train_loss=2.0587  test_loss=2.2077 | train_acc=0.2240  test_acc=0.2516\n",
      "Epoch 9/15  train_loss=2.0587  test_loss=2.2077 | train_acc=0.2240  test_acc=0.2516\n",
      "Epoch 10/15  train_loss=2.0480  test_loss=2.2027 | train_acc=0.2330  test_acc=0.2511\n",
      "Epoch 10/15  train_loss=2.0480  test_loss=2.2027 | train_acc=0.2330  test_acc=0.2511\n",
      "Epoch 11/15  train_loss=2.0303  test_loss=2.1990 | train_acc=0.2470  test_acc=0.2705\n",
      "Epoch 11/15  train_loss=2.0303  test_loss=2.1990 | train_acc=0.2470  test_acc=0.2705\n",
      "Epoch 12/15  train_loss=2.0165  test_loss=2.1901 | train_acc=0.2340  test_acc=0.2748\n",
      "Epoch 12/15  train_loss=2.0165  test_loss=2.1901 | train_acc=0.2340  test_acc=0.2748\n",
      "Epoch 13/15  train_loss=1.9925  test_loss=2.1825 | train_acc=0.2555  test_acc=0.2799\n",
      "Epoch 13/15  train_loss=1.9925  test_loss=2.1825 | train_acc=0.2555  test_acc=0.2799\n",
      "Epoch 14/15  train_loss=1.9737  test_loss=2.1701 | train_acc=0.2675  test_acc=0.3012\n",
      "Epoch 14/15  train_loss=1.9737  test_loss=2.1701 | train_acc=0.2675  test_acc=0.3012\n",
      "Epoch 15/15  train_loss=1.9602  test_loss=2.1663 | train_acc=0.2810  test_acc=0.3118\n",
      "2  \t1     \t1.37453\t1.17598\t2.16633\n",
      "Epoch 15/15  train_loss=1.9602  test_loss=2.1663 | train_acc=0.2810  test_acc=0.3118\n",
      "2  \t1     \t1.37453\t1.17598\t2.16633\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "False, False\n",
      "True, False\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "False, False\n",
      "True, False\n",
      "Epoch 1/15  train_loss=2.0348  test_loss=1.9221 | train_acc=0.2585  test_acc=0.3046\n",
      "Epoch 1/15  train_loss=2.0348  test_loss=1.9221 | train_acc=0.2585  test_acc=0.3046\n",
      "Epoch 2/15  train_loss=1.8172  test_loss=1.7673 | train_acc=0.3415  test_acc=0.3522\n",
      "Epoch 2/15  train_loss=1.8172  test_loss=1.7673 | train_acc=0.3415  test_acc=0.3522\n",
      "Epoch 3/15  train_loss=1.6758  test_loss=1.6146 | train_acc=0.3970  test_acc=0.4163\n",
      "Epoch 3/15  train_loss=1.6758  test_loss=1.6146 | train_acc=0.3970  test_acc=0.4163\n",
      "Epoch 4/15  train_loss=1.6191  test_loss=1.5679 | train_acc=0.4170  test_acc=0.4385\n",
      "Epoch 4/15  train_loss=1.6191  test_loss=1.5679 | train_acc=0.4170  test_acc=0.4385\n",
      "Epoch 5/15  train_loss=1.5551  test_loss=1.6840 | train_acc=0.4395  test_acc=0.4162\n",
      "Epoch 5/15  train_loss=1.5551  test_loss=1.6840 | train_acc=0.4395  test_acc=0.4162\n",
      "Epoch 6/15  train_loss=1.5054  test_loss=1.5431 | train_acc=0.4570  test_acc=0.4402\n",
      "Epoch 6/15  train_loss=1.5054  test_loss=1.5431 | train_acc=0.4570  test_acc=0.4402\n",
      "Epoch 7/15  train_loss=1.4691  test_loss=1.4724 | train_acc=0.4805  test_acc=0.4704\n",
      "Epoch 7/15  train_loss=1.4691  test_loss=1.4724 | train_acc=0.4805  test_acc=0.4704\n",
      "Epoch 8/15  train_loss=1.4325  test_loss=1.5072 | train_acc=0.4785  test_acc=0.4498\n",
      "Epoch 8/15  train_loss=1.4325  test_loss=1.5072 | train_acc=0.4785  test_acc=0.4498\n",
      "Epoch 9/15  train_loss=1.3593  test_loss=1.3994 | train_acc=0.5245  test_acc=0.5054\n",
      "Epoch 9/15  train_loss=1.3593  test_loss=1.3994 | train_acc=0.5245  test_acc=0.5054\n",
      "Epoch 10/15  train_loss=1.3302  test_loss=1.4755 | train_acc=0.5265  test_acc=0.4749\n",
      "Epoch 10/15  train_loss=1.3302  test_loss=1.4755 | train_acc=0.5265  test_acc=0.4749\n",
      "Epoch 11/15  train_loss=1.2993  test_loss=1.4864 | train_acc=0.5525  test_acc=0.4655\n",
      "Epoch 11/15  train_loss=1.2993  test_loss=1.4864 | train_acc=0.5525  test_acc=0.4655\n",
      "Epoch 12/15  train_loss=1.2873  test_loss=1.5573 | train_acc=0.5415  test_acc=0.4557\n",
      "Epoch 12/15  train_loss=1.2873  test_loss=1.5573 | train_acc=0.5415  test_acc=0.4557\n",
      "Epoch 13/15  train_loss=1.2265  test_loss=1.3905 | train_acc=0.5635  test_acc=0.5087\n",
      "Epoch 13/15  train_loss=1.2265  test_loss=1.3905 | train_acc=0.5635  test_acc=0.5087\n",
      "Epoch 14/15  train_loss=1.2139  test_loss=1.3239 | train_acc=0.5670  test_acc=0.5332\n",
      "Epoch 14/15  train_loss=1.2139  test_loss=1.3239 | train_acc=0.5670  test_acc=0.5332\n",
      "Epoch 15/15  train_loss=1.2150  test_loss=1.3698 | train_acc=0.5755  test_acc=0.5224\n",
      "Epoch 15/15  train_loss=1.2150  test_loss=1.3698 | train_acc=0.5755  test_acc=0.5224\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "3  \t4     \t1.21474\t1.17598\t1.36977\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "3  \t4     \t1.21474\t1.17598\t1.36977\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "4  \t2     \t1.17598\t1.17598\t1.17598\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "4  \t2     \t1.17598\t1.17598\t1.17598\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "5  \t2     \t1.17598\t1.17598\t1.17598\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "5  \t2     \t1.17598\t1.17598\t1.17598\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "6  \t2     \t1.17598\t1.17598\t1.17598\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "6  \t2     \t1.17598\t1.17598\t1.17598\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "7  \t2     \t1.17598\t1.17598\t1.17598\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "7  \t2     \t1.17598\t1.17598\t1.17598\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "8  \t2     \t1.17598\t1.17598\t1.17598\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "8  \t2     \t1.17598\t1.17598\t1.17598\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "9  \t2     \t1.17598\t1.17598\t1.17598\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "9  \t2     \t1.17598\t1.17598\t1.17598\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 1/15  train_loss=1.9507  test_loss=1.6837 | train_acc=0.2875  test_acc=0.3865\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 2/15  train_loss=1.6921  test_loss=1.5438 | train_acc=0.3920  test_acc=0.4486\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 3/15  train_loss=1.6042  test_loss=1.5057 | train_acc=0.4160  test_acc=0.4617\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 4/15  train_loss=1.5237  test_loss=1.4476 | train_acc=0.4655  test_acc=0.4823\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 5/15  train_loss=1.4408  test_loss=1.3896 | train_acc=0.5050  test_acc=0.5064\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 6/15  train_loss=1.3677  test_loss=1.3607 | train_acc=0.5140  test_acc=0.5174\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 7/15  train_loss=1.3541  test_loss=1.3091 | train_acc=0.5205  test_acc=0.5416\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 8/15  train_loss=1.3077  test_loss=1.3320 | train_acc=0.5470  test_acc=0.5229\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 9/15  train_loss=1.2709  test_loss=1.2804 | train_acc=0.5515  test_acc=0.5530\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 10/15  train_loss=1.2367  test_loss=1.2295 | train_acc=0.5680  test_acc=0.5648\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 11/15  train_loss=1.1735  test_loss=1.2550 | train_acc=0.5830  test_acc=0.5586\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 12/15  train_loss=1.1300  test_loss=1.2526 | train_acc=0.6090  test_acc=0.5601\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 13/15  train_loss=1.1139  test_loss=1.1917 | train_acc=0.6070  test_acc=0.5787\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 14/15  train_loss=1.1015  test_loss=1.2198 | train_acc=0.6260  test_acc=0.5741\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "10 \t2     \t1.17598\t1.17598\t1.17598\n",
      "Best individual (hof): [16, 7.777955714720123e-05, 0.6093114430769566, 1, 0, 1, 0]\n",
      "Best fitness (hof): (1.1759813084602355,)\n",
      "Epoch 15/15  train_loss=1.0731  test_loss=1.1760 | train_acc=0.6370  test_acc=0.5830\n",
      "10 \t2     \t1.17598\t1.17598\t1.17598\n",
      "Best individual (hof): [16, 7.777955714720123e-05, 0.6093114430769566, 1, 0, 1, 0]\n",
      "Best fitness (hof): (1.1759813084602355,)\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "num_epochs=15, sample_count=2000\n",
      "16, 7.777955714720123e-05, 0.6093114430769566\n",
      "True, False\n",
      "True, False\n",
      "Epoch 1/15  train_loss=1.9642  test_loss=1.7012 | train_acc=0.3010  test_acc=0.3938\n",
      "Epoch 1/15  train_loss=1.9642  test_loss=1.7012 | train_acc=0.3010  test_acc=0.3938\n",
      "Epoch 2/15  train_loss=1.7389  test_loss=1.5862 | train_acc=0.3880  test_acc=0.4396\n",
      "Epoch 2/15  train_loss=1.7389  test_loss=1.5862 | train_acc=0.3880  test_acc=0.4396\n",
      "Epoch 3/15  train_loss=1.6031  test_loss=1.5278 | train_acc=0.4245  test_acc=0.4630\n",
      "Epoch 3/15  train_loss=1.6031  test_loss=1.5278 | train_acc=0.4245  test_acc=0.4630\n",
      "Epoch 4/15  train_loss=1.5249  test_loss=1.4226 | train_acc=0.4615  test_acc=0.4950\n",
      "Epoch 4/15  train_loss=1.5249  test_loss=1.4226 | train_acc=0.4615  test_acc=0.4950\n",
      "Epoch 5/15  train_loss=1.4753  test_loss=1.3743 | train_acc=0.4800  test_acc=0.5064\n",
      "Epoch 5/15  train_loss=1.4753  test_loss=1.3743 | train_acc=0.4800  test_acc=0.5064\n",
      "Epoch 6/15  train_loss=1.3627  test_loss=1.4327 | train_acc=0.5180  test_acc=0.4861\n",
      "Epoch 6/15  train_loss=1.3627  test_loss=1.4327 | train_acc=0.5180  test_acc=0.4861\n",
      "Epoch 7/15  train_loss=1.3385  test_loss=1.3445 | train_acc=0.5300  test_acc=0.5237\n",
      "Epoch 7/15  train_loss=1.3385  test_loss=1.3445 | train_acc=0.5300  test_acc=0.5237\n",
      "Epoch 8/15  train_loss=1.3026  test_loss=1.3337 | train_acc=0.5535  test_acc=0.5267\n",
      "Epoch 8/15  train_loss=1.3026  test_loss=1.3337 | train_acc=0.5535  test_acc=0.5267\n",
      "Epoch 9/15  train_loss=1.2323  test_loss=1.2704 | train_acc=0.5660  test_acc=0.5501\n",
      "Epoch 9/15  train_loss=1.2323  test_loss=1.2704 | train_acc=0.5660  test_acc=0.5501\n",
      "Epoch 10/15  train_loss=1.2350  test_loss=1.2664 | train_acc=0.5800  test_acc=0.5472\n",
      "Epoch 10/15  train_loss=1.2350  test_loss=1.2664 | train_acc=0.5800  test_acc=0.5472\n",
      "Epoch 11/15  train_loss=1.2015  test_loss=1.2221 | train_acc=0.5760  test_acc=0.5643\n",
      "Epoch 11/15  train_loss=1.2015  test_loss=1.2221 | train_acc=0.5760  test_acc=0.5643\n",
      "Epoch 12/15  train_loss=1.1322  test_loss=1.2754 | train_acc=0.6105  test_acc=0.5541\n",
      "Epoch 12/15  train_loss=1.1322  test_loss=1.2754 | train_acc=0.6105  test_acc=0.5541\n",
      "Epoch 13/15  train_loss=1.1202  test_loss=1.3947 | train_acc=0.6145  test_acc=0.5149\n",
      "Epoch 13/15  train_loss=1.1202  test_loss=1.3947 | train_acc=0.6145  test_acc=0.5149\n",
      "Epoch 14/15  train_loss=1.0903  test_loss=1.3133 | train_acc=0.6210  test_acc=0.5438\n",
      "Epoch 14/15  train_loss=1.0903  test_loss=1.3133 | train_acc=0.6210  test_acc=0.5438\n",
      "Epoch 15/15  train_loss=1.0599  test_loss=1.2027 | train_acc=0.6315  test_acc=0.5747\n",
      "Re-evaluated best_cfg test_loss: 1.2026558881759644\n",
      "Epoch 15/15  train_loss=1.0599  test_loss=1.2027 | train_acc=0.6315  test_acc=0.5747\n",
      "Re-evaluated best_cfg test_loss: 1.2026558881759644\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from deap import algorithms, tools\n",
    "\n",
    "# 确保前面的 cell 都已运行（定义了 toolbox/evaluate/device 等）\n",
    "pop = toolbox.population(n=5)                # 小规模测试\n",
    "hof = tools.HallOfFame(1)\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "stats.register(\"avg\", np.mean)\n",
    "stats.register(\"min\", np.min)\n",
    "stats.register(\"max\", np.max)\n",
    "\n",
    "pop, logbook = algorithms.eaSimple(\n",
    "    pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=10,\n",
    "    stats=stats, halloffame=hof, verbose=True,\n",
    ")\n",
    "\n",
    "# 强制确保 hof 与最终种群一致（有时因评估不确定或复制问题会出现差异）\n",
    "hof.update(pop)\n",
    "print(\"Best individual (hof):\", hof[0])\n",
    "print(\"Best fitness (hof):\", hof[0].fitness.values)\n",
    "\n",
    "# 另外，重新用确定性 seed 对最佳个体做一次复评，得到可重复的参考值\n",
    "best_seed = int(abs(hash(tuple(hof[0]))) % (2**32))\n",
    "best_cfg = decode(hof[0])\n",
    "reval = train_model(best_cfg, num_epochs=15, id=\"hof_reval\", seed=best_seed)\n",
    "print(\"Re-evaluated best_cfg test_loss:\", reval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "377bf719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Population fitness list ===\n",
      "pop[0]: [16, 7.777955714720123e-05, 0.6093114430769566, 1, 0, 1, 0]  fitness.values=(1.1759813084602355,)  type=<class 'tuple'>  inner-type=<class 'float'>\n",
      "pop[1]: [16, 7.777955714720123e-05, 0.6093114430769566, 1, 0, 1, 0]  fitness.values=(1.1759813084602355,)  type=<class 'tuple'>  inner-type=<class 'float'>\n",
      "pop[2]: [16, 7.777955714720123e-05, 0.6093114430769566, 1, 0, 1, 0]  fitness.values=(1.1759813084602355,)  type=<class 'tuple'>  inner-type=<class 'float'>\n",
      "pop[3]: [16, 7.777955714720123e-05, 0.6093114430769566, 1, 0, 1, 0]  fitness.values=(1.1759813084602355,)  type=<class 'tuple'>  inner-type=<class 'float'>\n",
      "pop[4]: [16, 7.777955714720123e-05, 0.6093114430769566, 1, 0, 1, 0]  fitness.values=(1.1759813084602355,)  type=<class 'tuple'>  inner-type=<class 'float'>\n",
      "\n",
      "=== logbook last entries ===\n",
      "logbook last gen min: 1.1759813084602355\n",
      "logbook last gen avg: 1.1759813084602355\n",
      "logbook last gen max: 1.1759813084602355\n",
      "\n",
      "=== hof ===\n",
      "hof[0]: [16, 7.777955714720123e-05, 0.6093114430769566, 1, 0, 1, 0]\n",
      "hof[0].fitness.values: (1.1759813084602355,) type: <class 'tuple'> inner type: <class 'float'>\n",
      "\n",
      "Computed pop min: 1.1759813084602355 computed pop avg: 1.1759813084602355\n"
     ]
    }
   ],
   "source": [
    "# 诊断：打印当前 population 与 hof、logbook 的对比\n",
    "# 假设变量名为 pop, hof, logbook（与你的 notebook 一致）\n",
    "print(\"=== Population fitness list ===\")\n",
    "for i, ind in enumerate(pop):\n",
    "    val = ind.fitness.values\n",
    "    print(f\"pop[{i}]: {ind}  fitness.values={val}  type={type(val)}  inner-type={(type(val[0]) if len(val)>0 else None)}\")\n",
    "\n",
    "print(\"\\n=== logbook last entries ===\")\n",
    "if 'logbook' in globals():\n",
    "    print(\"logbook last gen min:\", list(logbook.select('min'))[-1])\n",
    "    print(\"logbook last gen avg:\", list(logbook.select('avg'))[-1])\n",
    "    print(\"logbook last gen max:\", list(logbook.select('max'))[-1])\n",
    "\n",
    "print(\"\\n=== hof ===\")\n",
    "print(\"hof[0]:\", hof[0])\n",
    "print(\"hof[0].fitness.values:\", hof[0].fitness.values, \"type:\", type(hof[0].fitness.values), \"inner type:\", type(hof[0].fitness.values[0]))\n",
    "\n",
    "# 手动从 population 计算最小值，确认 logbook 的 min 与 population 的实际 min 是否一致\n",
    "pop_mins = [float(ind.fitness.values[0]) for ind in pop]\n",
    "print(\"\\nComputed pop min:\", min(pop_mins), \"computed pop avg:\", sum(pop_mins)/len(pop_mins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "404f263d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed generations: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Min list sample (前3): [1.3169, 1.1854, 1.1991]\n",
      "Avg list sample (前3): [1.5151, 1.2642, 1.2761]\n",
      "Max list sample (前3): [1.7434, 1.522, 1.3357]\n",
      "Evals per gen: [12, 12, 11, 11, 11, 12, 12, 11, 11, 11]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 解析 GA 日志并绘图（改进版）\n",
    "import re, matplotlib.pyplot as plt\n",
    "\n",
    "log_text = \"\"\"\n",
    "Gen 1/10  evals=12  min=1.3169  avg=1.5151  max=1.7434\n",
    "Gen 2/10  evals=12  min=1.1854  avg=1.2642  max=1.5220\n",
    "Gen 3/10  evals=11  min=1.1991  avg=1.2761  max=1.3357\n",
    "Gen 4/10  evals=11  min=1.1991  avg=1.2687  max=1.3185\n",
    "Gen 5/10  evals=11  min=1.1991  avg=1.2545  max=1.3413\n",
    "Gen 6/10  evals=12  min=1.1187  avg=1.1972  max=1.2970\n",
    "Gen 7/10  evals=12  min=1.2058  avg=1.2512  max=1.3200\n",
    "Gen 8/10  evals=11  min=1.2016  avg=1.2629  max=1.5399\n",
    "Gen 9/10  evals=11  min=1.2016  avg=1.2090  max=1.2359\n",
    "Gen 10/10 evals=11  min=1.1800  avg=1.2050  max=1.2600\n",
    "\"\"\".strip()  # 补上第10代示例数据（如果真实日志缺失可替换成真实行）\n",
    "\n",
    "pattern = re.compile(r\"Gen (\\d+)/(\\d+)\\s+evals=(\\d+)\\s+min=([0-9.]+)\\s+avg=([0-9.]+)\\s+max=([0-9.]+)\")\n",
    "gens, mins_, avgs_, maxs_, evals_ = [], [], [], [], []\n",
    "for line in log_text.splitlines():\n",
    "    m = pattern.search(line)\n",
    "    if not m:\n",
    "        print(\"未匹配行: \", line)\n",
    "        continue\n",
    "    g, total, ev, mn, av, mx = m.groups()\n",
    "    gens.append(int(g))\n",
    "    mins_.append(float(mn))\n",
    "    avgs_.append(float(av))\n",
    "    maxs_.append(float(mx))\n",
    "    evals_.append(int(ev))\n",
    "\n",
    "print(\"Parsed generations:\", gens)\n",
    "print(\"Min list sample (前3):\", mins_[:3])\n",
    "print(\"Avg list sample (前3):\", avgs_[:3])\n",
    "print(\"Max list sample (前3):\", maxs_[:3])\n",
    "print(\"Evals per gen:\", evals_)\n",
    "\n",
    "if not gens:\n",
    "    raise ValueError(\"未解析到任何代数据，请检查 log_text 格式或正则 pattern。\")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(gens, mins_, marker='o', label='min')\n",
    "plt.plot(gens, avgs_, marker='o', label='avg')\n",
    "plt.plot(gens, maxs_, marker='o', label='max', alpha=0.5)\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('test_loss')\n",
    "plt.title('GA fitness curve (test_loss)')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "# 根据数据自动设置 y 轴范围，留一点边距\n",
    "y_all = mins_ + avgs_ + maxs_\n",
    "ylim_min = min(y_all) - 0.05\n",
    "ylim_max = max(y_all) + 0.05\n",
    "plt.ylim(ylim_min, ylim_max)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
