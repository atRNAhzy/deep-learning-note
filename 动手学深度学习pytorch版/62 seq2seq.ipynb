{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af6a954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd13b5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(d2l.Encoder):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # 嵌入层\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers,\n",
    "                          dropout=dropout)\n",
    "    \n",
    "    def forward(self, X, *args):\n",
    "        X = self.embedding(X)\n",
    "        X = X.permute(1, 0, 2)\n",
    "        output , state = self.rnn(X)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a0d1f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7, 4, 16]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
    "                      num_layers=2)\n",
    "encoder.eval()\n",
    "X = torch.zeros((4, 7), dtype=torch.long)\n",
    "output, state = encoder(X)\n",
    "output.shape, state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3405861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(d2l.Decoder):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,\n",
    "                          dropout=dropout)\n",
    "        \n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        return enc_outputs[1]  # outputs == (output, state), outputs[1] == state\n",
    "    \n",
    "    def forward(self, X, state):\n",
    "        # 结合decoder输入和隐藏状态信息\n",
    "        X = self.embedding(X).permute(1, 0, 2) # target, 目标语言序列输入\n",
    "        context = state[-1].repeat(X.shape[0], 1, 1) # 当前最新的隐藏状态，重复n次用于和X拼接        \n",
    "        X_and_context = torch.cat((X, context), 2)\n",
    "        \n",
    "        # RNN 层，生输出和隐藏状态\n",
    "        output, state = self.rnn(X_and_context, state)\n",
    "        # 通过一个线性层，将输出重新变为输入，即X和state的状态，用于下一次计算\n",
    "        output = self.dense(output).permute(1, 0, 2)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ed25ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 7, 10]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
    "                         num_layers=2)\n",
    "\n",
    "decoder.eval()\n",
    "state = decoder.init_state(encoder(X))\n",
    "output, state = decoder(X, state)\n",
    "output.shape, state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42e6be28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [4, 5, 0]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sequence_mask(X, valid_len, value=0):\n",
    "    maxlen = X.size(1)\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                        device=X.device)[None, :] < valid_len[:, None]\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "X = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "sequence_mask(X, torch.tensor([1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a7f347",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b58b13",
   "metadata": {},
   "source": [
    "（优雅，太优雅了）\n",
    "\n",
    "**函数的作用**\n",
    "\n",
    "这个函数的目的是：**将序列中超出有效长度的部分设置为指定值（通常是0）**\n",
    "\n",
    "比如你有一个批次的句子，但每个句子长度不同，需要用padding填充到相同长度，然后用mask标记哪些位置是有效的。\n",
    "\n",
    "1. 理解输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12c59ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设输入：\n",
    "X = torch.tensor([[1, 2, 3], \n",
    "                  [4, 5, 6]])        # 形状: (2, 3) - 2个序列，每个长度3\n",
    "valid_len = torch.tensor([1, 2])     # 第1个序列有效长度1，第2个序列有效长度2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddf83be",
   "metadata": {},
   "source": [
    "2. 获取最大长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1aab804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = X.size(1)\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4510d1f4",
   "metadata": {},
   "source": [
    "\n",
    "3. 创建位置索引（核心技巧）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a9c80de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(maxlen, dtype=torch.float32, device=X.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7636e5a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加上 [None, :] 变成行向量\n",
    "torch.arange(maxlen, dtype=torch.float32, device=X.device)[None, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ec30d1",
   "metadata": {},
   "source": [
    "4. 处理有效长度（另一个核心技巧）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7255ad40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将 tensor([1, 2]) 变成列向量\n",
    "valid_len[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47ff5cb",
   "metadata": {},
   "source": [
    "5. 广播比较（最精彩的部分！）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797660db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False],\n",
       "        [ True,  True, False]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 展开来看：\n",
    "# tensor([[0., 1., 2.]])  <  tensor([[1],\n",
    "#                                   [2]])\n",
    "# \n",
    "# 广播后比较：\n",
    "# [[0., 1., 2.]]  <  [[1, 1, 1],      # 第1行：每个位置都与1比较\n",
    "#                     [2, 2, 2]]      # 第2行：每个位置都与2比较\n",
    "mask = torch.arange(maxlen)[None, :] < valid_len[:, None]\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896f61a8",
   "metadata": {},
   "source": [
    "6. 应用掩码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9eb039d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[~mask] = 0  # ~mask 表示取反，即False的位置设为value\n",
    "X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca18a09",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70c35760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  1.,  1.,  1.],\n",
       "         [-1., -1., -1., -1.],\n",
       "         [-1., -1., -1., -1.]],\n",
       "\n",
       "        [[ 1.,  1.,  1.,  1.],\n",
       "         [ 1.,  1.,  1.,  1.],\n",
       "         [-1., -1., -1., -1.]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones(2, 3, 4)\n",
    "sequence_mask(X, torch.tensor([1, 2]), value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59eb5367",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskSoftmaxCEloss(nn.CrossEntropyLoss):\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        weights = torch.ones_like(label)\n",
    "        weights = sequence_mask(weights, valid_len)\n",
    "        self.reduction = 'none' # 有mask，所以不能做reduction\n",
    "        unweighted_loss = super(MaskSoftmaxCEloss, self).forward(\n",
    "            pred.permute(0, 2, 1), label) \n",
    "        # 换顺序是因为 nn.CrossEntropyLoss 要求的输入类型是 (mini-batch, 类别, 维度)\n",
    "        weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
    "        return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95f59601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3026, 1.1513, 0.0000])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = MaskSoftmaxCEloss()\n",
    "loss(torch.ones(3, 4, 10), torch.ones((3, 4), dtype=torch.long),\n",
    "     torch.tensor([4, 2, 0]))\n",
    "\n",
    "# label是3个长为4的序列，mask最后一位为0，所以最后一个序列完全没有计算loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c015e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
    "        return self.decoder(dec_X, dec_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1eb0c964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq(net, data_iter, lr, num_eopchs, tgt_vocab, device):\n",
    "    def xavier_init_weights(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        if type(m) == nn.GRU:\n",
    "            for param in m._flat_weights_names:\n",
    "                # 只初始化权重，不初始化偏置（置零）\n",
    "                # 偏置在GRU中包含意义：\n",
    "                # 重置门（R）：初始时不倾向与重置或保留\n",
    "                # 更新门（Z）：初始时不倾向于更新或保持\n",
    "                if \"weight\" in param:\n",
    "                    nn.init.xavier_uniform_(m._parameters[param])\n",
    "    net.apply(xavier_init_weights)\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss = MaskSoftmaxCEloss()\n",
    "    net.train()\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[10, num_eopchs])\n",
    "    for epoch in range(num_eopchs):\n",
    "        timer = d2l.Timer()\n",
    "        metric = d2l.Accumulator(2)\n",
    "        for batch in data_iter:\n",
    "            optimizer.zero_grad()\n",
    "            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
    "            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],\n",
    "                               device=device).reshape(-1, 1)    \n",
    "            dec_input = torch.cat([bos, Y[:, :-1]], 1) # 强制教学，在真实目标上训练，忽略错误的预测值\n",
    "            Y_hat, _ = net(X, dec_input, X_valid_len)\n",
    "            l = loss(Y_hat, Y, Y_valid_len)\n",
    "            l.sum().backward()\n",
    "            d2l.grad_clipping(net, 1)\n",
    "            num_tokens = Y_valid_len.sum()  # 需要计算有效tokens的数量，不然无法计算平均损失             \n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l.sum(), num_tokens)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            animator.add(epoch + 1, (metric[0] / metric[1],))\n",
    "    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\n",
    "          f'tokens/sec on {str(device)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e19a7deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.019, 21583.2 tokens/sec on cuda:0\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"262.1875pt\" height=\"183.35625pt\" viewBox=\"0 0 262.1875 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-07-16T20:59:16.582301</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 183.35625 \n",
       "L 262.1875 183.35625 \n",
       "L 262.1875 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 50.14375 145.8 \n",
       "L 245.44375 145.8 \n",
       "L 245.44375 7.2 \n",
       "L 50.14375 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 77.081681 145.8 \n",
       "L 77.081681 7.2 \n",
       "\" clip-path=\"url(#p5cf66695ef)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path id=\"mf9a38009ce\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf9a38009ce\" x=\"77.081681\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 50 -->\n",
       "      <g transform=\"translate(70.719181 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 110.754095 145.8 \n",
       "L 110.754095 7.2 \n",
       "\" clip-path=\"url(#p5cf66695ef)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf9a38009ce\" x=\"110.754095\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 100 -->\n",
       "      <g transform=\"translate(101.210345 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 144.426509 145.8 \n",
       "L 144.426509 7.2 \n",
       "\" clip-path=\"url(#p5cf66695ef)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf9a38009ce\" x=\"144.426509\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 150 -->\n",
       "      <g transform=\"translate(134.882759 160.398438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 178.098922 145.8 \n",
       "L 178.098922 7.2 \n",
       "\" clip-path=\"url(#p5cf66695ef)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf9a38009ce\" x=\"178.098922\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(168.555172 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 211.771336 145.8 \n",
       "L 211.771336 7.2 \n",
       "\" clip-path=\"url(#p5cf66695ef)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf9a38009ce\" x=\"211.771336\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 250 -->\n",
       "      <g transform=\"translate(202.227586 160.398438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 245.44375 145.8 \n",
       "L 245.44375 7.2 \n",
       "\" clip-path=\"url(#p5cf66695ef)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf9a38009ce\" x=\"245.44375\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 300 -->\n",
       "      <g transform=\"translate(235.9 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_7\">\n",
       "     <!-- epoch -->\n",
       "     <g transform=\"translate(132.565625 174.076563) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 50.14375 118.670761 \n",
       "L 245.44375 118.670761 \n",
       "\" clip-path=\"url(#p5cf66695ef)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <defs>\n",
       "       <path id=\"m1d4350f986\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1d4350f986\" x=\"50.14375\" y=\"118.670761\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.05 -->\n",
       "      <g transform=\"translate(20.878125 122.46998) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path d=\"M 50.14375 85.181305 \n",
       "L 245.44375 85.181305 \n",
       "\" clip-path=\"url(#p5cf66695ef)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1d4350f986\" x=\"50.14375\" y=\"85.181305\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.10 -->\n",
       "      <g transform=\"translate(20.878125 88.980524) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <path d=\"M 50.14375 51.691849 \n",
       "L 245.44375 51.691849 \n",
       "\" clip-path=\"url(#p5cf66695ef)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1d4350f986\" x=\"50.14375\" y=\"51.691849\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.15 -->\n",
       "      <g transform=\"translate(20.878125 55.491068) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <path d=\"M 50.14375 18.202393 \n",
       "L 245.44375 18.202393 \n",
       "\" clip-path=\"url(#p5cf66695ef)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1d4350f986\" x=\"50.14375\" y=\"18.202393\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.20 -->\n",
       "      <g transform=\"translate(20.878125 22.001612) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_12\">\n",
       "     <!-- loss -->\n",
       "     <g transform=\"translate(14.798437 86.157813) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_21\">\n",
       "    <path d=\"M 50.14375 13.5 \n",
       "L 56.878233 54.110141 \n",
       "L 63.612716 79.731104 \n",
       "L 70.347198 95.08828 \n",
       "L 77.081681 105.92472 \n",
       "L 83.816164 113.833163 \n",
       "L 90.550647 119.538853 \n",
       "L 97.285129 124.60757 \n",
       "L 104.019612 127.702261 \n",
       "L 110.754095 130.386883 \n",
       "L 117.488578 131.8819 \n",
       "L 124.22306 133.163002 \n",
       "L 130.957543 133.564499 \n",
       "L 137.692026 135.285254 \n",
       "L 144.426509 135.77266 \n",
       "L 151.160991 136.51928 \n",
       "L 157.895474 137.208932 \n",
       "L 164.629957 137.912774 \n",
       "L 171.36444 137.363409 \n",
       "L 178.098922 137.915373 \n",
       "L 184.833405 138.370519 \n",
       "L 191.567888 138.336174 \n",
       "L 198.302371 138.844234 \n",
       "L 205.036853 138.891979 \n",
       "L 211.771336 139.065439 \n",
       "L 218.505819 138.880482 \n",
       "L 225.240302 139.206599 \n",
       "L 231.974784 139.5 \n",
       "L 238.709267 139.145241 \n",
       "L 245.44375 139.395315 \n",
       "\" clip-path=\"url(#p5cf66695ef)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 50.14375 145.8 \n",
       "L 50.14375 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 245.44375 145.8 \n",
       "L 245.44375 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 50.14375 145.8 \n",
       "L 245.44375 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 50.14375 7.2 \n",
       "L 245.44375 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p5cf66695ef\">\n",
       "   <rect x=\"50.14375\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
    "batch_size, num_steps = 64, 10\n",
    "lr, num_epochs, device = 0.005, 300, d2l.try_gpu()\n",
    "\n",
    "train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)\n",
    "encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers,\n",
    "                         dropout)\n",
    "decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers,\n",
    "                         dropout)\n",
    "net = EncoderDecoder(encoder, decoder)\n",
    "train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f8456de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,\n",
    "                    device, save_attention_weights=False):\n",
    "    \"\"\"\n",
    "    seq2seq模型预测函数 - 将源语言句子翻译成目标语言\n",
    "    \n",
    "    参数:\n",
    "    - net: 训练好的seq2seq模型（包含encoder和decoder）\n",
    "    - src_sentence: 源语言句子（字符串），例如 \"hello world\"\n",
    "    - src_vocab: 源语言词汇表，用于将词转换为ID\n",
    "    - tgt_vocab: 目标语言词汇表，用于将ID转换为词\n",
    "    - num_steps: 最大生成长度，防止无限生成\n",
    "    - device: 计算设备（cpu或gpu）\n",
    "    - save_attention_weights: 是否保存注意力权重（当前版本未使用）\n",
    "    \n",
    "    返回:\n",
    "    - 翻译后的目标语言句子（字符串）\n",
    "    - 注意力权重序列（当前为空列表）\n",
    "    \"\"\"\n",
    "    \n",
    "    # === 第一阶段：模型准备 ===\n",
    "    # 设置模型为评估模式，关闭dropout和batch normalization的训练行为\n",
    "    net.eval()\n",
    "    \n",
    "    # === 第二阶段：源句子预处理 ===\n",
    "    # 步骤：字符串 → 词列表 → ID列表 → 添加结束符\n",
    "    # 例如: \"hello world\" → [\"hello\", \"world\"] → [vocab_id1, vocab_id2] → [vocab_id1, vocab_id2, eos_id]\n",
    "    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [src_vocab['<eos>']]\n",
    "    # src_vocab[word_list] 会自动将词列表转换为对应的ID列表\n",
    "    # 添加<eos>是为了明确标记源序列的结束\n",
    "    \n",
    "    # 记录原始源序列的真实长度（用于encoder处理时的mask）\n",
    "    enc_valid_len = torch.tensor([len(src_tokens)], device=device)\n",
    "    # 形状: (1,) - 只有一个句子，所以长度为1的tensor\n",
    "    \n",
    "    # 对源序列进行填充或截断，统一长度为num_steps\n",
    "    # 如果序列太短，用<pad>填充；如果太长，进行截断\n",
    "    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n",
    "    # 现在src_tokens的长度固定为num_steps\n",
    "    \n",
    "    # === 第三阶段：准备encoder输入 ===\n",
    "    # 将Python列表转换为PyTorch tensor，并添加batch维度\n",
    "    # 从形状 [seq_len] 变成 [1, seq_len]，因为batch_size=1\n",
    "    enc_X = torch.unsqueeze(\n",
    "        torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)\n",
    "    # enc_X形状: (1, num_steps) - 1个句子，长度为num_steps\n",
    "    \n",
    "    # === 第四阶段：编码器处理 ===\n",
    "    # 编码器将源语言序列编码成语义表示（隐藏状态）\n",
    "    enc_outputs = net.encoder(enc_X, enc_valid_len)\n",
    "    # enc_outputs是一个元组: (encoder_output, encoder_state)\n",
    "    # - encoder_output: 每个时间步的输出 (seq_len, 1, hidden_size)\n",
    "    # - encoder_state: 最终隐藏状态 (num_layers, 1, hidden_size)\n",
    "    \n",
    "    # === 第五阶段：初始化解码器 ===\n",
    "    # 用编码器的最终状态来初始化解码器的隐藏状态\n",
    "    # 这样解码器就能\"知道\"源句子的语义信息\n",
    "    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
    "    # dec_state就是encoder的最终隐藏状态，包含了源句子的语义信息\n",
    "    \n",
    "    # 准备解码器的第一个输入：<bos>（开始符）\n",
    "    # 解码器需要知道\"现在开始生成目标语言\"\n",
    "    dec_X = torch.unsqueeze(torch.tensor(\n",
    "        [tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0)\n",
    "    # dec_X形状: (1, 1) - batch_size=1, seq_len=1，内容是<bos>的token_id\n",
    "    \n",
    "    # === 第六阶段：准备输出容器 ===\n",
    "    output_seq = []              # 存储生成的词ID序列\n",
    "    attention_weight_seq = []    # 存储注意力权重（当前版本未使用）\n",
    "    \n",
    "    # === 第七阶段：自回归解码（核心生成过程）===\n",
    "    # \"自回归\"：用已生成的词来预测下一个词\n",
    "    for _ in range(num_steps):  # 最多生成num_steps个词，防止无限循环\n",
    "        \n",
    "        # 7.1 解码器前向传播\n",
    "        # 输入：当前词(dec_X) + 隐藏状态(dec_state)\n",
    "        # 输出：下一个词的概率分布 + 更新的隐藏状态\n",
    "        Y, dec_state = net.decoder(dec_X, dec_state)\n",
    "        # Y: 词汇表上的概率分布 (1, 1, vocab_size)\n",
    "        # dec_state: 更新后的隐藏状态，包含到目前为止的所有信息\n",
    "        \n",
    "        # 7.2 贪心选择：选择概率最大的词\n",
    "        # 在实际应用中，也可以用beam search等更复杂的搜索策略\n",
    "        dec_X = Y.argmax(dim=2)  # 在词汇表维度(dim=2)上找最大值的索引\n",
    "        # Y: (1, 1, vocab_size) → dec_X: (1, 1)，内容是预测词的token_id\n",
    "        \n",
    "        # 7.3 提取预测词的ID（转换为Python整数）\n",
    "        pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n",
    "        # 操作分解：\n",
    "        # squeeze(dim=0): (1, 1) → (1,) - 去掉batch维度\n",
    "        # type(torch.int32): 转换数据类型确保兼容性\n",
    "        # item(): 从单元素tensor中提取Python整数\n",
    "        \n",
    "        # 7.4 保存注意力权重（如果需要可视化的话）\n",
    "        # 注意：当前的基础seq2seq没有attention机制，所以这部分不会执行\n",
    "        if save_attention_weights:\n",
    "            attention_weight_seq.append(net.decoder.attention_weights)\n",
    "        \n",
    "        # 7.5 检查是否到达序列结束\n",
    "        # 如果生成了<eos>（结束符），说明翻译完成\n",
    "        if pred == tgt_vocab['<eos>']:\n",
    "            break  # 提前结束生成循环\n",
    "        \n",
    "        # 7.6 将预测词添加到输出序列\n",
    "        output_seq.append(pred)\n",
    "        # output_seq现在包含了从开始到当前生成的所有词的ID\n",
    "        \n",
    "        # 重要：dec_X会在下一轮循环中作为decoder的输入\n",
    "        # 这就是\"自回归\"的核心：用刚生成的词来生成下一个词\n",
    "    \n",
    "    # === 第八阶段：后处理和返回结果 ===\n",
    "    # 将词ID序列转换回可读的文本\n",
    "    # tgt_vocab.to_tokens(): 将ID列表 [1, 5, 9] 转换为词列表 [\"I\", \"love\", \"you\"]\n",
    "    # ' '.join(): 将词列表用空格连接成句子 \"I love you\"\n",
    "    translated_sentence = ' '.join(tgt_vocab.to_tokens(output_seq))\n",
    "    \n",
    "    # 返回翻译结果和注意力权重（用于可视化分析）\n",
    "    return translated_sentence, attention_weight_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5612f91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu(pred_seq, label_seq, k):\n",
    "    \"\"\"\n",
    "    BLEU (Bilingual Evaluation Understudy) 评分函数\n",
    "    用于评估机器翻译质量，分数越高表示翻译质量越好\n",
    "    \n",
    "    参数:\n",
    "    - pred_seq: 模型预测的翻译结果（字符串），例如 \"I love you\"\n",
    "    - label_seq: 标准参考翻译（字符串），例如 \"I adore you\"  \n",
    "    - k: 计算到k-gram的匹配度，通常k=4\n",
    "    \n",
    "    返回:\n",
    "    - BLEU分数（0到1之间的浮点数）\n",
    "    \"\"\"\n",
    "    \n",
    "    # === 第一步：文本预处理 ===\n",
    "    # 将句子按空格分割成词列表\n",
    "    pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')\n",
    "    # 例如: \"I love you\" → [\"I\", \"love\", \"you\"]\n",
    "    # 例如: \"I adore you\" → [\"I\", \"adore\", \"you\"]\n",
    "    \n",
    "    # 计算预测序列和参考序列的长度\n",
    "    len_pred, len_label = len(pred_tokens), len(label_tokens)\n",
    "    # 例如: len_pred = 3, len_label = 3\n",
    "    \n",
    "    # === 第二步：长度惩罚项（Brevity Penalty）===\n",
    "    # BLEU会惩罚过短的翻译，避免模型生成很短但高精度的句子来\"作弊\"\n",
    "    score = math.exp(min(0, 1 - len_label / len_pred))\n",
    "    # 分析这个公式：\n",
    "    # - 如果 len_pred >= len_label：1 - len_label/len_pred >= 0，min()取0，exp(0)=1，无惩罚\n",
    "    # - 如果 len_pred < len_label：1 - len_label/len_pred < 0，exp()>1，但min()取0，bleu减小\n",
    "\n",
    "    \n",
    "    # === 第三步：n-gram精确度计算 ===\n",
    "    # 对于每个n-gram级别（1-gram, 2-gram, ..., k-gram）\n",
    "    for n in range(1, k + 1):  # n = 1, 2, 3, ..., k\n",
    "        \n",
    "        # 初始化计数器\n",
    "        num_matches = 0  # 匹配的n-gram数量\n",
    "        label_subs = collections.defaultdict(int)  # 参考序列中n-gram的计数字典\n",
    "        \n",
    "        # === 3.1：统计参考序列中所有n-gram ===\n",
    "        # 遍历参考序列，提取所有可能的n-gram\n",
    "        for i in range(len_label - n + 1):\n",
    "            # 提取从位置i开始的n个连续词\n",
    "            ngram = ' '.join(label_tokens[i: i + n])\n",
    "            # 例如当n=2，i=0: label_tokens[0:2] → \"I adore\"\n",
    "            # 例如当n=2，i=1: label_tokens[1:3] → \"adore you\"\n",
    "            \n",
    "            # 在字典中记录这个n-gram出现的次数\n",
    "            label_subs[ngram] += 1\n",
    "        # 此时label_subs包含了参考序列中所有n-gram及其出现次数\n",
    "        \n",
    "        # === 3.2：检查预测序列中的n-gram匹配情况 ===\n",
    "        # 遍历预测序列，检查每个n-gram是否在参考序列中出现\n",
    "        for i in range(len_pred - n + 1):\n",
    "            # 提取从位置i开始的n个连续词\n",
    "            ngram = ' '.join(pred_tokens[i: i + n])\n",
    "            # 例如当n=2，i=0: pred_tokens[0:2] → \"I love\"\n",
    "            \n",
    "            # 检查这个n-gram是否在参考序列中存在且还有剩余计数\n",
    "            if label_subs[ngram] > 0:\n",
    "                num_matches += 1        # 找到一个匹配，计数+1\n",
    "                label_subs[ngram] -= 1  # 消耗掉参考序列中的一个计数\n",
    "                # 这样做是为了避免重复匹配：\n",
    "                # 如果预测序列中有多个相同的n-gram，但参考序列中只有1个，\n",
    "                # 那么只能匹配1次，不能重复计分\n",
    "        \n",
    "        # === 3.3：计算当前n-gram级别的精确度 ===\n",
    "        # 精确度 = 匹配的n-gram数量 / 预测序列中总的n-gram数量\n",
    "        precision_n = num_matches / (len_pred - n + 1)\n",
    "        # 例如：2-gram精确度 = 匹配的2-gram数 / 预测序列中2-gram总数\n",
    "        \n",
    "        # === 3.4：将当前精确度加权融入总分 ===\n",
    "        # BLEU使用几何平均，每个n-gram级别的权重是 0.5^n\n",
    "        weight = math.pow(0.5, n)  # n越大，权重越小\n",
    "        # n=1: weight=0.5, n=2: weight=0.25, n=3: weight=0.125, n=4: weight=0.0625\n",
    "        \n",
    "        # 使用对数形式计算几何平均：log(a*b) = log(a) + log(b)\n",
    "        # 这里用乘法实现：score *= precision_n^weight\n",
    "        score *= math.pow(precision_n, weight)\n",
    "    \n",
    "    # === 第四步：返回最终BLEU分数 ===\n",
    "    return score\n",
    "    # BLEU分数范围：0-1，越接近1表示翻译质量越好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e85bb5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go . => va !, bleu 1.000000\n",
      "i lost . => j'ai <unk> perdu ., bleu 0.658037\n",
      "he's calm . => il est riche ., bleu 0.658037\n",
      "i'm home . => je suis détendu !, bleu 0.418438\n"
     ]
    }
   ],
   "source": [
    "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
    "for eng, fra in zip(engs, fras):\n",
    "    translation, attention_weight_seq = predict_seq2seq(\n",
    "        net, eng, src_vocab, tgt_vocab, num_steps, device)\n",
    "    print(f'{eng} => {translation}, bleu {bleu(translation, fra, k=2):3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
